\documentclass[runningheads]{llncs}

% TODO FINAL: Comment out review mode for camera-ready.
\usepackage[review,year=2026,ID=*****]{eccv}
%\usepackage{eccv}

\usepackage{eccvabbrv}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[accsupp]{axessibility}

% TODO FINAL: switch to non-review hyperref option.
\usepackage[pagebackref,breaklinks,colorlinks,citecolor=eccvblue]{hyperref}
%\usepackage{hyperref}

\begin{document}

\title{CERA: Causal Event Reasoning and Attribution for Real-Time Surveillance}
\titlerunning{CERA}

\author{Anonymous Authors}
\authorrunning{Anonymous Authors}
\institute{Anonymous Institution\\
\email{anonymous@eccv2026.org}}

\maketitle

\begin{abstract}
Real-time surveillance systems need more than anomaly flags or descriptive captions.
Operational response requires causal accounts that connect events, agents, and outcomes under strict latency constraints.
We introduce \textbf{CERA} (\textbf{C}ausal \textbf{E}vent \textbf{R}easoning and \textbf{A}ttribution), a framework direction for online surveillance analysis that prioritizes three requirements: causal fidelity, evidential grounding, and runtime efficiency.
This manuscript presents an initial formalization with a structured objective, an online output contract, and an evaluation protocol that jointly considers reasoning quality and deployment-time feasibility.
\keywords{Causal Event Reasoning, Attribution, Surveillance, Efficient Inference}
\end{abstract}

\section{Introduction}

\paragraph{Why Causality Matters in Surveillance.}
Modern surveillance models can detect anomalies or generate captions, but practical response requires more: a causal account of what happened, what triggered it, and which entities were responsible.
In safety-critical settings, this distinction affects intervention priority, accountability, and prevention planning.

\paragraph{Problem Setting.}
We consider online surveillance streams where decisions must be both timely and explainable.
For each event segment, the system should produce (1) an event statement, (2) an attribution statement linking causes to outcomes, and (3) supporting evidence references.
We refer to this objective as \textit{causal event reasoning and attribution}.

\paragraph{Current Gap.}
Existing pipelines are often optimized for a single objective~\cite{vad_survey,video_surveillance_survey}.
Detection-focused systems emphasize recall but provide limited causal structure~\cite{rtfm,mist,vadclip,anomalygpt,holmes_vad}.
Caption-focused systems can describe scenes fluently but often blur causality and agency~\cite{video_chatgpt,videollava,moviechat}.
Offline reasoning approaches are expressive but difficult to deploy with strict real-time constraints~\cite{timechat,moviechat}.

\paragraph{CERA.}
We introduce \textbf{CERA}, a framework direction for real-time causal event reasoning and attribution.
CERA is organized around three constraints: \textit{causal fidelity} (correct cause-effect structure), \textit{evidential grounding} (traceable support in observed frames), and \textit{runtime efficiency} (practical throughput for continuous streams).
This paper starts by fixing these requirements as first-class design targets.

\paragraph{Scope of This Draft.}
This manuscript is built from a clean-room start.
The current version includes method-level formalization, an implementation-ready reference stack, and a projected-results package for experiment planning.
All quantitative tables in this draft are explicitly non-empirical placeholders and will be replaced by measured values after external-server execution.

\paragraph{Contributions.}
\begin{itemize}
    \item We define a real-time surveillance task for causal event reasoning and attribution with explicit outputs for event, cause, and evidence.
    \item We propose CERA as a framework direction centered on causal fidelity, evidential grounding, and runtime efficiency.
    \item We provide a full evaluation template with projected quantitative targets to guide external empirical runs.
\end{itemize}

\section{Related Work}

\paragraph{Surveillance Anomaly Detection.}
Prior surveillance literature has made strong progress on anomaly detection under weak supervision, including benchmark construction and learning objectives that improve anomaly recall~\cite{vad_survey,ucfcrime,xdviolence,rtfm,mist}.
More recent adaptations of vision-language models improve semantic coverage for anomaly understanding~\cite{vadclip,anomalygpt,holmes_vad}.
However, most systems still optimize anomaly scoring or descriptive diagnosis, not explicit cause-outcome attribution with verifiable evidence links.

\paragraph{Video-Language Reasoning.}
General video-language systems provide richer temporal descriptions and dialogue-style interaction~\cite{video_chatgpt,videollava,timechat,moviechat}.
These advances are valuable for open-ended understanding, but they are not primarily designed around causal direction consistency, evidence attachment, or deployment-time abstention behavior required in online surveillance settings.

\paragraph{Position of CERA.}
CERA is not another captioning or anomaly-score variant.
It targets the missing intersection between surveillance deployment constraints and causal attribution requirements by enforcing structured outputs, evidence checks, and runtime-feasibility objectives in one contract.

\section{Method}

\subsection{Task Formulation}
Let a surveillance stream be $\mathcal{V}=\{\mathbf{X}_1,\mathbf{X}_2,\dots\}$, where $\mathbf{X}_t$ is the frame at time $t$.
The system identifies event windows $\mathcal{W}=\{[t_s^k,t_e^k]\}_{k=1}^{K}$ and outputs one structured report per window.

For each window $k$, CERA predicts
\begin{equation}
\hat{\mathbf{y}}_k = \{\hat{e}_k, \hat{\mathcal{A}}_k, \hat{\mathcal{E}}_k\},
\end{equation}
where $\hat{e}_k$ is the event statement, $\hat{\mathcal{A}}_k$ is a set of causal attribution tuples, and $\hat{\mathcal{E}}_k$ is an evidence index set.

Each attribution tuple is represented as
\begin{equation}
\hat{a}_{k,j}=(\hat{c}_{k,j}, \hat{r}_{k,j}, \hat{o}_{k,j}),
\end{equation}
where $\hat{c}_{k,j}$ is a candidate cause, $\hat{o}_{k,j}$ is an outcome, and $\hat{r}_{k,j}$ is the causal relation type.
The evidence set $\hat{\mathcal{E}}_k$ maps each tuple to supporting temporal spans or frame references.

\subsection{CERA Design Requirements}
CERA is designed around three joint requirements.

\paragraph{Causal Fidelity.}
Predictions should preserve cause-effect direction and avoid descriptive but non-causal statements.
Operationally, this means the attribution tuples should match reference causal structure, not only lexical overlap.

\paragraph{Evidential Grounding.}
Every attribution claim should be linked to observable evidence in the same event window.
Ungrounded claims are treated as low-trust outputs regardless of language fluency.

\paragraph{Runtime Efficiency.}
Inference must satisfy deployment latency constraints under continuous streams.
Given an end-to-end latency $L_{e2e}$ and deployment budget $L_{max}$, CERA should operate in the feasible region:
\begin{equation}
L_{e2e} \leq L_{max}.
\end{equation}

\subsection{Structured Objective}
We model CERA as maximizing a utility that balances causal quality, evidence quality, and runtime feasibility:
\begin{equation}
\mathcal{U} = \lambda_c S_c + \lambda_e S_e + \lambda_r S_r,
\end{equation}
where $S_c$ measures causal fidelity, $S_e$ measures evidence grounding quality, and $S_r$ measures runtime fitness.

For runtime fitness, we use a normalized score:
\begin{equation}
S_r = \min\left(1,\frac{L_{max}}{L_{e2e}}\right).
\end{equation}
This formulation encourages quality gains only when latency remains practical.

\subsection{Online Inference Contract}
Given an event window $\mathbf{X}_{t_s^k:t_e^k}$, CERA predicts:
\begin{equation}
\hat{\mathbf{y}}_k = \mathcal{F}_{\theta}\!\left(\mathbf{X}_{t_s^k:t_e^k}\right),
\end{equation}
with confidence score $p_k$.
To reduce high-confidence hallucination risk in safety contexts, CERA uses abstention:
\begin{equation}
\hat{\mathbf{y}}_k =
\begin{cases}
\mathcal{F}_{\theta}(\mathbf{X}_{t_s^k:t_e^k}) & \text{if } p_k \ge \tau_{conf},\\
\varnothing & \text{otherwise}.
\end{cases}
\end{equation}

This contract makes failure modes explicit and prevents forcing a causal narrative when evidence is weak.

\subsection{Output Schema and Validation}
For deployment integration, CERA exposes a fixed output schema:
\begin{equation}
\hat{\mathbf{y}}_k = \{\hat{e}_k,\hat{\mathcal{A}}_k,\hat{\mathcal{E}}_k,\hat{s}_k\},
\end{equation}
where $\hat{s}_k$ is a quality status flag (\texttt{valid}, \texttt{abstain}, or \texttt{insufficient\_evidence}).

Before returning \texttt{valid}, CERA applies causal and evidence validators.
For each predicted tuple $\hat{a}_{k,j}=(\hat{c}_{k,j},\hat{r}_{k,j},\hat{o}_{k,j})$, we define:
\begin{equation}
\mathbb{I}^{rel}_{k,j}=\mathbf{1}\!\left[\hat{r}_{k,j}\in\mathcal{R}_{allow}\right],\quad
\mathbb{I}^{dir}_{k,j}=\mathbf{1}\!\left[t(\hat{c}_{k,j})\le t(\hat{o}_{k,j})+\delta_{dir}\right],
\end{equation}
\begin{equation}
\mathbb{I}^{ent}_{k,j}=\mathbf{1}\!\left[\hat{c}_{k,j},\hat{o}_{k,j}\in\mathcal{V}^{track}_{k}\right].
\end{equation}
The causal validator is:
\begin{equation}
\text{CausalCheck}(\hat{\mathcal{A}}_k)=\prod_{j}\left(\mathbb{I}^{rel}_{k,j}\mathbb{I}^{dir}_{k,j}\mathbb{I}^{ent}_{k,j}\right).
\end{equation}

For evidence links $\hat{\mathcal{E}}_{k,j}$ and support scores $q_{k,j}$:
\begin{equation}
\mathbb{I}^{sup}_{k,j}=\mathbf{1}\!\left[q_{k,j}\ge\tau_{evd}\right],\quad
\mathbb{I}^{link}_{k,j}=\mathbf{1}\!\left[|\hat{\mathcal{E}}_{k,j}|\ge1\right],
\end{equation}
with
\begin{equation}
\text{EvidenceCheck}(\hat{\mathcal{A}}_k,\hat{\mathcal{E}}_k)=\prod_{j}\left(\mathbb{I}^{sup}_{k,j}\mathbb{I}^{link}_{k,j}\right).
\end{equation}

Final status is determined by confidence, validation, and runtime budget:
\begin{equation}
\hat{s}_k=
\begin{cases}
\texttt{valid}, & \text{if }\begin{array}[t]{l}
p_k\ge\tau_{conf},\ \text{CausalCheck}=1,\\
\text{EvidenceCheck}=1,\ L_{e2e}^{k}\le L_{max}
\end{array}\\
\texttt{insufficient\_evidence}, & \text{if }\begin{array}[t]{l}
p_k\ge\tau_{conf},\ L_{e2e}^{k}\le L_{max},\\
(\text{CausalCheck}=0\lor \text{EvidenceCheck}=0)
\end{array}\\
\texttt{abstain}, & \text{otherwise}.
\end{cases}
\end{equation}
This policy makes safety behavior explicit: uncertain or ungrounded claims are not promoted to \texttt{valid}.

\subsection{Failure Taxonomy}
To keep failure analysis actionable, we partition output errors into three types:
\begin{itemize}
    \item \textbf{Causal Structure Violation}: cause-effect direction or relation type is wrong.
    \item \textbf{Evidence Support Failure}: claim is plausible but unsupported by linked evidence.
    \item \textbf{Runtime Budget Breach}: quality is acceptable but latency violates deployment budget.
\end{itemize}
This taxonomy maps directly to the three CERA requirements and guides where to improve: reasoning logic, evidence alignment, or system optimization.

\subsection{Module-Level Design}
We decompose CERA into four cooperative modules:
\begin{equation}
\hat{\mathbf{y}}_k =
\mathcal{M}_{ctrl}\!\left(
\mathcal{M}_{evd}\!\left(
\mathcal{M}_{attr}\!\left(
\mathcal{M}_{evt}\!\left(\mathbf{X}_{t_s^k:t_e^k}\right)\right)\right)\right).
\end{equation}
This decomposition keeps responsibilities explicit and aligns each module with one or more CERA requirements.

\paragraph{Event Proposal Module ($\mathcal{M}_{evt}$).}
The event proposal stage generates candidate temporal windows and tracked entities from the stream.
Its goal is high recall under bounded latency, producing a compact event context for downstream reasoning instead of processing the full stream at maximum cost.

\paragraph{Attribution Construction Module ($\mathcal{M}_{attr}$).}
Given event context, CERA builds a directed causal interaction graph
\begin{equation}
\mathcal{G}_k=(\mathcal{V}_k,\mathcal{E}_k^{c}),
\end{equation}
where nodes represent entities/outcomes and directed edges represent candidate causal relations.
Final attribution tuples are decoded from validated graph edges, enforcing direction consistency by construction.

\paragraph{Evidence Alignment Module ($\mathcal{M}_{evd}$).}
For each candidate tuple, CERA links supporting evidence spans in the same event window.
Each claim receives an evidence support score $q_{k,j}$ and is retained only if $q_{k,j}\ge\tau_{evd}$.
This stage prevents fluent but ungrounded attributions from passing as valid outputs.

\paragraph{Budget-Aware Control Module ($\mathcal{M}_{ctrl}$).}
The control module manages compute allocation and output policy under runtime budgets.
It applies adaptive depth, selective decoding, and abstention triggers to keep latency within $L_{max}$ while preserving causal and evidential quality.
When budget pressure is high, the controller prioritizes reliable partial outputs over unstable full attributions.

\subsection{Reference Instantiation (CERA-Ref)}
To make CERA implementation-ready, we define a reference stack (\textbf{CERA-Ref}) with concrete but replaceable components.

\paragraph{Event Backbone.}
CERA-Ref supports two detector backbones with different deployment priorities:
(1) a lightweight one-stage detector (YOLOv8-Nano class) for strict latency budgets, and
(2) a transformer detector (DETR-R50 class) for stronger global scene matching.
These detector families are representative of efficient one-stage and global matching paradigms~\cite{yolo,detr}.
For frame $\mathbf{X}_t$, the detector returns boxes $\mathcal{B}_t$ and risk scores $\mathcal{S}_t$, and triggers downstream reasoning only when the event score exceeds a gate threshold.

\paragraph{Reasoning Backbone.}
For attribution generation, CERA-Ref uses an open-source instruction-tuned 7B VLM with a 336px visual encoder setting.
In our reference profile, this corresponds to a LLaVA-family backbone~\cite{llava,llava15}.
This choice keeps the reasoning backbone reproducible while preserving enough capacity for causal language outputs.

\paragraph{Detection-Guided Token Compaction.}
Before attribution decoding, CERA-Ref applies detection-guided visual token selection:
\begin{equation}
\mathcal{T}^{keep}_t = \mathcal{T}(\mathcal{R}_t)\cup \mathcal{T}\!\left(\text{Dilate}(\mathcal{R}_t,\alpha)\right),
\end{equation}
where $\mathcal{R}_t$ is the detector-derived ROI set and $\alpha$ is a context dilation factor.
This keeps hazard-centric tokens and a thin context ring while pruning background-heavy regions.

\paragraph{Budgeted Decoding.}
During generation, CERA-Ref constrains active KV usage by a budget ratio $\rho_{kv}$:
\begin{equation}
K_{active} = \left\lceil \rho_{kv}\cdot K_{full}\right\rceil, \quad 0<\rho_{kv}\le1.
\end{equation}
The controller selects the top-$K_{active}$ entries via query-conditioned saliency and falls back to abstention or partial report if quality checks fail.

\subsection{Optimization Strategy}
CERA-Ref follows a staged optimization schedule that separates recall, reasoning, and runtime control.

\paragraph{Stage 1: Event Proposal Calibration.}
The detector is tuned with risk-aware weighting:
\begin{equation}
\mathcal{L}_{evt}=\sum_{c}\lambda_c\mathcal{L}^{(c)}_{det},
\end{equation}
where higher-risk classes use larger $\lambda_c$ to preserve recall on safety-critical events.

\paragraph{Stage 2: Attribution and Evidence Learning.}
Given triggered windows, the reasoning module optimizes attribution and evidence objectives jointly:
\begin{equation}
\mathcal{L}_{reason}=\mathcal{L}_{attr}+\gamma\mathcal{L}_{evd}.
\end{equation}
$\mathcal{L}_{attr}$ supervises relation direction/type and tuple content, while $\mathcal{L}_{evd}$ supervises support validity for each tuple.

\paragraph{Stage 3: Budget Controller Tuning.}
Controller parameters $(\tau_{conf},\tau_{evd},\rho_{kv})$ are tuned against deployment constraints by maximizing utility under latency limits:
\begin{equation}
\max \mathcal{U}\quad \text{s.t.}\quad L_{e2e}\le L_{max}.
\end{equation}
This stage determines the operating point between conservative abstention and aggressive throughput.

\subsection{Online Inference Procedure}
At deployment time, CERA-Ref runs the following sequence for each incoming stream window:
\begin{enumerate}
    \item run event proposal and gate non-event windows early,
    \item build attribution candidates from retained event context,
    \item attach evidence spans and compute support scores,
    \item apply schema and consistency checks,
    \item enforce runtime budget policy to output \texttt{valid}, \texttt{insufficient\_evidence}, or \texttt{abstain}.
\end{enumerate}
This pipeline operationalizes CERA as a deterministic contract rather than free-form generation.

\subsection{Current Scope}
This manuscript now defines CERA at task and objective levels with concrete module design.
It also specifies a reference instantiation for implementation and a projected result profile for planning.
The remaining stage is external-server execution that replaces projected tables with measured outcomes.

\section{Experiments}

\paragraph{Draft Status Notice.}
All quantitative values in this section are \textbf{projected} for planning and discussion only.
No claims in this section are from executed runs yet; real values will be filled after external-server experiments.

\subsection{Evaluation Targets}
We evaluate CERA along three axes aligned with the method objective:
\begin{itemize}
    \item \textbf{Causal Quality}: correctness of predicted attribution tuples.
    \item \textbf{Evidence Quality}: correctness and completeness of linked evidence references.
    \item \textbf{Runtime}: end-to-end latency and effective throughput under fixed hardware.
\end{itemize}
Benchmark design is based on established surveillance anomaly datasets~\cite{ucfcrime,xdviolence} with additional attribution/evidence labels.

\subsection{Benchmark and Annotation Protocol}
For each video, we derive event windows with fixed temporal stride and event-aware boundary checks.
Each evaluation window is annotated with:
\begin{itemize}
    \item event statement target $e^{*}$,
    \item attribution tuple set $\mathcal{A}^{*}$ with directed relation labels,
    \item evidence span set $\mathcal{E}^{*}$ linking tuples to frame ranges.
\end{itemize}
To reduce annotation drift, each sample is reviewed by two annotators and adjudicated under a fixed causal direction guideline.
For planning-scale projection, we set the benchmark size as follows.
\begin{table}[t]
\centering
\caption{Projected benchmark scale for planning (non-empirical).}
\label{tab:projected_dataset}
\begin{tabular}{lccc}
\toprule
Dataset & Train windows & Val windows & Test windows \\
\midrule
UCF-Crime derived & 1,920 & 640 & 960 \\
XD-Violence derived & 2,560 & 768 & 1,024 \\
\bottomrule
\end{tabular}
\end{table}
The same plan assumes projected annotation agreement of $\kappa=0.81$ for relation direction and span-level overlap agreement of 0.74 for evidence links.

\subsection{Baseline Families}
We compare CERA against three baseline groups:
\begin{itemize}
    \item \textbf{Detection-centric}: surveillance anomaly baselines focused on event scoring~\cite{rtfm,mist}.
    \item \textbf{VLM-adapted anomaly systems}: models that couple language outputs with anomaly detection~\cite{vadclip,anomalygpt,holmes_vad}.
    \item \textbf{Video-language reasoning}: general long-video reasoning models~\cite{video_chatgpt,videollava}.
    We additionally include long-context variants~\cite{timechat,moviechat}.
\end{itemize}
Within CERA, we report profile variants (YOLO-class and DETR-class detectors) under identical downstream attribution/evidence modules.

\subsection{Metric Definitions}
Let $\hat{\mathcal{A}}$ and $\mathcal{A}^{*}$ be predicted and reference attribution tuples.
Tuple matching requires aligned cause, outcome, and relation direction.
With matched pair set $\mathcal{M}_{A}$:
\begin{equation}
P_A=\frac{|\mathcal{M}_{A}|}{|\hat{\mathcal{A}}|},\quad
R_A=\frac{|\mathcal{M}_{A}|}{|\mathcal{A}^{*}|},\quad
F1_A=\frac{2P_AR_A}{P_A+R_A}.
\end{equation}

For evidence, let $\hat{\mathcal{E}}$ and $\mathcal{E}^{*}$ denote predicted and reference support links.
We compute support precision/recall/F1 analogously as $P_E,R_E,F1_E$.
Runtime is reported by latency, throughput, and budget-feasibility rate:
\begin{equation}
R_{feas}=\frac{1}{K}\sum_{k=1}^{K}\mathbf{1}\!\left[L_{e2e}^{k}\le L_{max}\right].
\end{equation}
To evaluate status-policy behavior, we also report:
\begin{equation}
R_{valid}=\frac{1}{K}\sum_{k=1}^{K}\mathbf{1}\!\left[\hat{s}_{k}=\texttt{valid}\right],
\end{equation}
\begin{equation}
R_{unsafe}=\frac{\sum_{k=1}^{K}\mathbf{1}\!\left[\hat{s}_{k}=\texttt{valid}\land (\text{TupleMatch}_{k}=0\lor \text{EvidenceMatch}_{k}=0)\right]}{\sum_{k=1}^{K}\mathbf{1}\!\left[\hat{s}_{k}=\texttt{valid}\right]+\epsilon}.
\end{equation}
Lower $R_{unsafe}$ indicates safer deployment-time behavior under the same coverage.
We additionally report mean utility $\bar{\mathcal{U}}$ to summarize quality--latency trade-offs in one score.

\subsection{Protocol Principles}
To keep comparisons meaningful, all compared systems are assigned the same runtime protocol:
hardware class, input resolution, decoding length, and reporting policy.
Detector-swap comparisons (YOLO-class vs DETR-class) keep the rest of the CERA pipeline fixed.

\subsection{Implementation and Reproducibility}
All reference settings are versioned in CERA configs.
We use `base.yaml` for the YOLO profile and `base\_detr.yaml` for the DETR profile.
The default profile uses YOLO-class event proposal and an open 7B VLM backbone; the DETR profile swaps only detector-related parameters and adjusted latency budget.
In the default reference profile, key control parameters are initialized as $\tau_{conf}=0.45$, $\tau_{evd}=0.55$, $\rho_{kv}=0.25$, and $L_{max}=120$ms.
For reproducibility, we fix random seeds, report configuration files used per run, and release per-video prediction dumps for attribution and evidence outputs.
The external execution target is a single high-memory GPU server with unchanged protocol settings.

\subsection{Statistical Reporting}
This draft reports projected point estimates only.
When measured runs become available, we will attach mean/std over multiple seeds, 95\% bootstrap confidence intervals for $F1_A$, $F1_E$, and $R_{feas}$, and paired significance tests on per-video metrics.

\subsection{Projected Main Results (Non-Empirical)}
Table~\ref{tab:projected_main} summarizes projected outcomes under the unified protocol.
% MEASURED_SWAP_START:main_results
\begin{table*}[t]
\centering
\caption{Projected main results for planning only (non-empirical). Higher is better for $F1_A$, $F1_E$, $R_{feas}$, $R_{valid}$; lower is better for latency and $R_{unsafe}$.}
\label{tab:projected_main}
\begin{tabular}{lcccccc}
\toprule
Method & $F1_A$ & $F1_E$ & $R_{feas}$ & Latency (ms) & $R_{valid}$ & $R_{unsafe}$ \\
\midrule
RTFM + template attribution & 24.1 & 18.7 & 0.97 & 34 & 0.88 & 0.43 \\
AnomalyGPT-style VLM adaptation & 39.8 & 31.4 & 0.63 & 182 & 0.71 & 0.27 \\
Video-LLaVA-style reasoning baseline & 42.7 & 34.5 & 0.58 & 205 & 0.69 & 0.25 \\
CERA-Ref (YOLO profile) & 55.9 & 49.2 & 0.91 & 108 & 0.74 & 0.11 \\
CERA-Ref (DETR profile) & 58.1 & 51.0 & 0.78 & 142 & 0.72 & 0.09 \\
\bottomrule
\end{tabular}
\end{table*}
% MEASURED_SWAP_END:main_results
The projected trend is that CERA-Ref improves attribution/evidence quality while preserving deployment feasibility, with DETR providing slightly higher quality at higher latency.

\subsection{Projected Ablations (Non-Empirical)}
We project component effects relative to full CERA-Ref (YOLO) in Table~\ref{tab:projected_ablation}.
% MEASURED_SWAP_START:ablation_results
\begin{table}[t]
\centering
\caption{Projected ablation deltas against full CERA-Ref (YOLO), planning only (non-empirical).}
\label{tab:projected_ablation}
\begin{tabular}{lcccc}
\toprule
Setting & $\Delta F1_A$ & $\Delta F1_E$ & $\Delta R_{feas}$ & $\Delta R_{unsafe}$ \\
\midrule
No Event Gating & +0.8 & +0.4 & -0.29 & +0.02 \\
No Token Compaction & +0.3 & +0.2 & -0.17 & +0.01 \\
No Budgeted Decoding & +1.0 & +0.7 & -0.36 & +0.00 \\
No Evidence Gate & +1.4 & -6.8 & +0.01 & +0.14 \\
\bottomrule
\end{tabular}
\end{table}
% MEASURED_SWAP_END:ablation_results
These projected deltas indicate that evidence gating is the most important safety component, while event gating and budgeted decoding are primary contributors to runtime feasibility.

\subsection{Planned Ablations}
To validate each design choice, we plan a component-wise ablation suite:
\begin{itemize}
    \item \textbf{Full CERA-Ref}: event proposal + attribution + evidence alignment + budget controller.
    \item \textbf{Detector Backbone Swap}: YOLO-class vs DETR-class under identical downstream settings.
    \item \textbf{No Event Gating}: process all windows to measure temporal-efficiency contribution.
    \item \textbf{No Token Compaction}: disable detection-guided token selection.
    \item \textbf{No Budgeted Decoding}: use dense decoding to isolate controller impact.
    \item \textbf{No Evidence Gate}: keep attribution without evidence-threshold filtering.
\end{itemize}
The external-server run will replace projected deltas with measured deltas and utility changes.

\subsection{Projected Error Breakdown (Non-Empirical)}
For failure analysis planning, we project error composition in invalid or downgraded cases as:
% MEASURED_SWAP_START:error_breakdown
\begin{itemize}
    \item \textbf{Evidence-link miss (46\%)}: tuples are generated without sufficient grounded support.
    \item \textbf{Causal direction inversion (29\%)}: cause/outcome order flipped under multi-agent interactions.
    \item \textbf{Entity-role ambiguity (25\%)}: interacting entities are detected but role assignment is unstable.
\end{itemize}
% MEASURED_SWAP_END:error_breakdown
This breakdown guides implementation priority for the first external run.

\section{Threats, Limitations, and Deployment Notes}

\paragraph{Projected-vs-measured gap.}
All tables in this draft are projected priors, not executed measurements.
Therefore, relative orderings and magnitudes in Section~4 may change after external-server execution.

\paragraph{Dataset transfer risk.}
Projected numbers assume that annotation quality and event taxonomy are consistent across collection domains.
Real deployments may present unseen camera placement, motion blur, and nighttime conditions that alter both attribution and evidence performance.

\paragraph{Annotation uncertainty.}
Causal tuple boundaries and evidence spans include inherent subjectivity even under adjudication rules.
Measured reporting will include agreement summaries and per-category error slices to separate model error from annotation ambiguity.

\paragraph{Operational deployment constraints.}
Latency feasibility depends on hardware profile, queue pressure, and stream fan-out.
The external runbook fixes these settings for reproducibility; production deviations should be audited against the same reporting contract.

\paragraph{Measured replacement protocol.}
When measured outputs are available, projected blocks marked by \texttt{MEASURED\_SWAP\_START/END} in Section~4 will be replaced first.
This minimizes narrative drift and keeps the main manuscript aligned with execution artifacts.

\section{Conclusion}
This paper established \textbf{CERA} as a practical objective for real-time surveillance understanding: produce causal event reports that are structurally correct, evidentially grounded, and operationally feasible.
We formalized this objective through (1) a structured task definition, (2) a utility that balances causal quality, grounding quality, and runtime fitness, (3) an online contract with abstention and explicit status outputs, and (4) a concrete module-level design for event proposal, attribution construction, evidence alignment, and budget-aware control.
We also completed a projected evaluation package with non-empirical benchmark scale, comparative tables, and ablation trends to guide execution.

Projected numbers indicate a consistent target pattern: stronger attribution/evidence quality than detection- or caption-centric baselines with controlled safety behavior ($R_{unsafe}$ reduction), while maintaining feasible latency in the YOLO profile.
These values are planning priors only.
The remaining step is to run the same protocol on an external high-compute server and replace each projected figure with measured evidence.

\bibliographystyle{splncs04}
\bibliography{main}

\end{document}
