\documentclass[runningheads]{llncs}

% TODO FINAL: Comment out review mode for camera-ready.
\usepackage[review,year=2026,ID=*****]{eccv}
%\usepackage{eccv}

\usepackage{eccvabbrv}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[accsupp]{axessibility}

% TODO FINAL: switch to non-review hyperref option.
\usepackage[pagebackref,breaklinks,colorlinks,citecolor=eccvblue]{hyperref}
%\usepackage{hyperref}

\begin{document}

\title{CERA: Causal Event Reasoning and Attribution for Real-Time Surveillance}
\titlerunning{CERA}

\author{Anonymous Authors}
\authorrunning{Anonymous Authors}
\institute{Anonymous Institution\\
\email{anonymous@eccv2026.org}}

\maketitle

\begin{abstract}
Real-time surveillance systems need more than anomaly flags or descriptive captions.
Operational response requires causal accounts that connect events, agents, and outcomes under strict latency constraints.
We introduce \textbf{CERA} (\textbf{C}ausal \textbf{E}vent \textbf{R}easoning and \textbf{A}ttribution), a framework direction for online surveillance analysis that prioritizes three requirements: causal fidelity, evidential grounding, and runtime efficiency.
This manuscript presents an initial formalization with a structured objective, an online output contract, and an evaluation protocol that jointly considers reasoning quality and deployment-time feasibility.
\keywords{Causal Event Reasoning, Attribution, Surveillance, Efficient Inference}
\end{abstract}

\section{Introduction}

\paragraph{Why Causality Matters in Surveillance.}
Modern surveillance models can detect anomalies or generate captions, but practical response requires more: a causal account of what happened, what triggered it, and which entities were responsible.
In safety-critical settings, this distinction affects intervention priority, accountability, and prevention planning.

\paragraph{Problem Setting.}
We consider online surveillance streams where decisions must be both timely and explainable.
For each event segment, the system should produce (1) an event statement, (2) an attribution statement linking causes to outcomes, and (3) supporting evidence references.
We refer to this objective as \textit{causal event reasoning and attribution}.

\paragraph{Current Gap.}
Existing pipelines are often optimized for a single objective.
Detection-focused systems emphasize recall but provide limited causal structure.
Caption-focused systems can describe scenes fluently but often blur causality and agency.
Offline reasoning approaches are expressive but difficult to deploy with strict real-time constraints.

\paragraph{CERA.}
We introduce \textbf{CERA}, a framework direction for real-time causal event reasoning and attribution.
CERA is organized around three constraints: \textit{causal fidelity} (correct cause-effect structure), \textit{evidential grounding} (traceable support in observed frames), and \textit{runtime efficiency} (practical throughput for continuous streams).
This paper starts by fixing these requirements as first-class design targets.

\paragraph{Scope of This Draft.}
This manuscript is built from a clean-room start.
The current version includes a first method-level formalization and protocol-level experiment design.
Next iterations will expand concrete module implementations and larger-scale empirical validation.

\paragraph{Contributions.}
\begin{itemize}
    \item We define a real-time surveillance task for causal event reasoning and attribution with explicit outputs for event, cause, and evidence.
    \item We propose CERA as a framework direction centered on causal fidelity, evidential grounding, and runtime efficiency.
    \item We establish a starting evaluation perspective that jointly considers causal quality and deployment-time performance.
\end{itemize}

\section{Method}

\subsection{Task Formulation}
Let a surveillance stream be $\mathcal{V}=\{\mathbf{X}_1,\mathbf{X}_2,\dots\}$, where $\mathbf{X}_t$ is the frame at time $t$.
The system identifies event windows $\mathcal{W}=\{[t_s^k,t_e^k]\}_{k=1}^{K}$ and outputs one structured report per window.

For each window $k$, CERA predicts
\begin{equation}
\hat{\mathbf{y}}_k = \{\hat{e}_k, \hat{\mathcal{A}}_k, \hat{\mathcal{E}}_k\},
\end{equation}
where $\hat{e}_k$ is the event statement, $\hat{\mathcal{A}}_k$ is a set of causal attribution tuples, and $\hat{\mathcal{E}}_k$ is an evidence index set.

Each attribution tuple is represented as
\begin{equation}
\hat{a}_{k,j}=(\hat{c}_{k,j}, \hat{r}_{k,j}, \hat{o}_{k,j}),
\end{equation}
where $\hat{c}_{k,j}$ is a candidate cause, $\hat{o}_{k,j}$ is an outcome, and $\hat{r}_{k,j}$ is the causal relation type.
The evidence set $\hat{\mathcal{E}}_k$ maps each tuple to supporting temporal spans or frame references.

\subsection{CERA Design Requirements}
CERA is designed around three joint requirements.

\paragraph{Causal Fidelity.}
Predictions should preserve cause-effect direction and avoid descriptive but non-causal statements.
Operationally, this means the attribution tuples should match reference causal structure, not only lexical overlap.

\paragraph{Evidential Grounding.}
Every attribution claim should be linked to observable evidence in the same event window.
Ungrounded claims are treated as low-trust outputs regardless of language fluency.

\paragraph{Runtime Efficiency.}
Inference must satisfy deployment latency constraints under continuous streams.
Given an end-to-end latency $L_{e2e}$ and deployment budget $L_{max}$, CERA should operate in the feasible region:
\begin{equation}
L_{e2e} \leq L_{max}.
\end{equation}

\subsection{Structured Objective}
We model CERA as maximizing a utility that balances causal quality, evidence quality, and runtime feasibility:
\begin{equation}
\mathcal{U} = \lambda_c S_c + \lambda_e S_e + \lambda_r S_r,
\end{equation}
where $S_c$ measures causal fidelity, $S_e$ measures evidence grounding quality, and $S_r$ measures runtime fitness.

For runtime fitness, we use a normalized score:
\begin{equation}
S_r = \min\left(1,\frac{L_{max}}{L_{e2e}}\right).
\end{equation}
This formulation encourages quality gains only when latency remains practical.

\subsection{Online Inference Contract}
Given an event window $\mathbf{X}_{t_s^k:t_e^k}$, CERA predicts:
\begin{equation}
\hat{\mathbf{y}}_k = \mathcal{F}_{\theta}\!\left(\mathbf{X}_{t_s^k:t_e^k}\right),
\end{equation}
with confidence score $p_k$.
To reduce high-confidence hallucination risk in safety contexts, CERA uses abstention:
\begin{equation}
\hat{\mathbf{y}}_k =
\begin{cases}
\mathcal{F}_{\theta}(\mathbf{X}_{t_s^k:t_e^k}) & \text{if } p_k \ge \tau_{conf},\\
\varnothing & \text{otherwise}.
\end{cases}
\end{equation}

This contract makes failure modes explicit and prevents forcing a causal narrative when evidence is weak.

\subsection{Output Schema and Validation}
For deployment integration, CERA exposes a fixed output schema:
\begin{equation}
\hat{\mathbf{y}}_k = \{\hat{e}_k,\hat{\mathcal{A}}_k,\hat{\mathcal{E}}_k,\hat{s}_k\},
\end{equation}
where $\hat{s}_k$ is a quality status flag (\texttt{valid}, \texttt{abstain}, or \texttt{insufficient\_evidence}).

Before returning \texttt{valid}, CERA applies two consistency checks:
\begin{equation}
\text{CausalCheck}(\hat{\mathcal{A}}_k)=1,\quad
\text{EvidenceCheck}(\hat{\mathcal{A}}_k,\hat{\mathcal{E}}_k)=1.
\end{equation}
The first check verifies direction-consistent tuples and relation admissibility.
The second check verifies whether each accepted tuple has linked observable support.
If either check fails, CERA downgrades output status to \texttt{insufficient\_evidence} and suppresses final attribution claims.

\subsection{Error Taxonomy}
To keep failure analysis actionable, we partition output errors into three types:
\begin{itemize}
    \item \textbf{Type-I (Structure Error)}: cause-effect direction or relation type is wrong.
    \item \textbf{Type-II (Grounding Error)}: claim is plausible but unsupported by linked evidence.
    \item \textbf{Type-III (Latency Error)}: quality is acceptable but latency violates deployment budget.
\end{itemize}
This taxonomy maps directly to the three CERA requirements and guides where to improve: reasoning logic, evidence alignment, or system optimization.

\subsection{Current Scope}
This version defines CERA at the task and objective level.
Next iterations will specify the concrete module design (event proposal, attribution construction, evidence alignment, and budget-aware control) and their implementation details.

\section{Experiments}

\subsection{Evaluation Targets}
We evaluate CERA along three axes aligned with the method objective:
\begin{itemize}
    \item \textbf{Causal Quality}: correctness of predicted attribution tuples.
    \item \textbf{Evidence Quality}: correctness and completeness of linked evidence references.
    \item \textbf{Runtime}: end-to-end latency and effective throughput under fixed hardware.
\end{itemize}

\subsection{Planned Metrics}
Let $\hat{\mathcal{A}}$ and $\mathcal{A}^{*}$ be predicted and reference attribution tuples, and let $\hat{\mathcal{E}}$ and $\mathcal{E}^{*}$ be predicted and reference evidence.
We plan to report:
\begin{itemize}
    \item tuple-level precision/recall/F1 for causal attribution,
    \item evidence precision/recall/F1 for grounding quality,
    \item end-to-end latency (ms), FPS, and stream capacity.
\end{itemize}

\subsection{Protocol Principles}
To keep comparisons meaningful, all compared systems will use the same runtime protocol:
hardware, input resolution, decoding length, and reporting policy.
Final claims will be tied to both quality metrics and latency feasibility under deployment budgets.

\section{Conclusion}
This paper established \textbf{CERA} as a practical objective for real-time surveillance understanding: produce causal event reports that are structurally correct, evidentially grounded, and operationally feasible.
We formalized this objective through (1) a structured task definition, (2) a utility that balances causal quality, grounding quality, and runtime fitness, and (3) an online contract with abstention and explicit status outputs.
We also defined an initial evaluation protocol aligned with these requirements.

The next validation stage is empirical.
We plan to instantiate concrete CERA modules, build annotated benchmarks for attribution and evidence linkage, and run controlled comparisons against detection- and caption-centric baselines under matched runtime settings.
This progression is intended to test whether causal reasoning quality can improve without violating deployment latency budgets.

\bibliographystyle{splncs04}
\bibliography{main}

\end{document}
