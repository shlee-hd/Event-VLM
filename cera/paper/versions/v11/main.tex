\documentclass[runningheads]{llncs}

% TODO FINAL: Comment out review mode for camera-ready.
\usepackage[review,year=2026,ID=*****]{eccv}
%\usepackage{eccv}

\usepackage{eccvabbrv}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[accsupp]{axessibility}

% TODO FINAL: switch to non-review hyperref option.
\usepackage[pagebackref,breaklinks,colorlinks,citecolor=eccvblue]{hyperref}
%\usepackage{hyperref}

\begin{document}

\title{CERA: Causal Event Reasoning and Attribution for Real-Time Surveillance}
\titlerunning{CERA}

\author{Anonymous Authors}
\authorrunning{Anonymous Authors}
\institute{Anonymous Institution\\
\email{anonymous@eccv2026.org}}

\maketitle

\begin{abstract}
Real-time surveillance systems need more than anomaly flags or descriptive captions.
Operational response requires causal accounts that connect events, agents, and outcomes under strict latency constraints.
We introduce \textbf{CERA} (\textbf{C}ausal \textbf{E}vent \textbf{R}easoning and \textbf{A}ttribution), a framework direction for online surveillance analysis that prioritizes three requirements: causal fidelity, evidential grounding, and runtime efficiency.
This manuscript presents an initial formalization with a structured objective, an online output contract, and an evaluation protocol that jointly considers reasoning quality and deployment-time feasibility.
\keywords{Causal Event Reasoning, Attribution, Surveillance, Efficient Inference}
\end{abstract}

\section{Introduction}

\paragraph{Why Causality Matters in Surveillance.}
Modern surveillance models can detect anomalies or generate captions, but practical response requires more: a causal account of what happened, what triggered it, and which entities were responsible.
In safety-critical settings, this distinction affects intervention priority, accountability, and prevention planning.

\paragraph{Problem Setting.}
We consider online surveillance streams where decisions must be both timely and explainable.
For each event segment, the system should produce (1) an event statement, (2) an attribution statement linking causes to outcomes, and (3) supporting evidence references.
We refer to this objective as \textit{causal event reasoning and attribution}.

\paragraph{Current Gap.}
Existing pipelines are often optimized for a single objective~\cite{vad_survey,video_surveillance_survey}.
Detection-focused systems emphasize recall but provide limited causal structure~\cite{rtfm,mist,vadclip,anomalygpt,holmes_vad}.
Caption-focused systems can describe scenes fluently but often blur causality and agency~\cite{video_chatgpt,videollava,moviechat}.
Offline reasoning approaches are expressive but difficult to deploy with strict real-time constraints~\cite{timechat,moviechat}.

\paragraph{CERA.}
We introduce \textbf{CERA}, a framework direction for real-time causal event reasoning and attribution.
CERA is organized around three constraints: \textit{causal fidelity} (correct cause-effect structure), \textit{evidential grounding} (traceable support in observed frames), and \textit{runtime efficiency} (practical throughput for continuous streams).
This paper starts by fixing these requirements as first-class design targets.

\paragraph{Scope of This Draft.}
This manuscript is built from a clean-room start.
The current version includes a first method-level formalization and protocol-level experiment design.
Next iterations will expand concrete module implementations and larger-scale empirical validation.

\paragraph{Contributions.}
\begin{itemize}
    \item We define a real-time surveillance task for causal event reasoning and attribution with explicit outputs for event, cause, and evidence.
    \item We propose CERA as a framework direction centered on causal fidelity, evidential grounding, and runtime efficiency.
    \item We establish a starting evaluation perspective that jointly considers causal quality and deployment-time performance.
\end{itemize}

\section{Related Work}

\paragraph{Surveillance Anomaly Detection.}
Prior surveillance literature has made strong progress on anomaly detection under weak supervision, including benchmark construction and learning objectives that improve anomaly recall~\cite{vad_survey,ucfcrime,xdviolence,rtfm,mist}.
More recent adaptations of vision-language models improve semantic coverage for anomaly understanding~\cite{vadclip,anomalygpt,holmes_vad}.
However, most systems still optimize anomaly scoring or descriptive diagnosis, not explicit cause-outcome attribution with verifiable evidence links.

\paragraph{Video-Language Reasoning.}
General video-language systems provide richer temporal descriptions and dialogue-style interaction~\cite{video_chatgpt,videollava,timechat,moviechat}.
These advances are valuable for open-ended understanding, but they are not primarily designed around causal direction consistency, evidence attachment, or deployment-time abstention behavior required in online surveillance settings.

\paragraph{Position of CERA.}
CERA is not another captioning or anomaly-score variant.
It targets the missing intersection between surveillance deployment constraints and causal attribution requirements by enforcing structured outputs, evidence checks, and runtime-feasibility objectives in one contract.

\section{Method}

\subsection{Task Formulation}
Let a surveillance stream be $\mathcal{V}=\{\mathbf{X}_1,\mathbf{X}_2,\dots\}$, where $\mathbf{X}_t$ is the frame at time $t$.
The system identifies event windows $\mathcal{W}=\{[t_s^k,t_e^k]\}_{k=1}^{K}$ and outputs one structured report per window.

For each window $k$, CERA predicts
\begin{equation}
\hat{\mathbf{y}}_k = \{\hat{e}_k, \hat{\mathcal{A}}_k, \hat{\mathcal{E}}_k\},
\end{equation}
where $\hat{e}_k$ is the event statement, $\hat{\mathcal{A}}_k$ is a set of causal attribution tuples, and $\hat{\mathcal{E}}_k$ is an evidence index set.

Each attribution tuple is represented as
\begin{equation}
\hat{a}_{k,j}=(\hat{c}_{k,j}, \hat{r}_{k,j}, \hat{o}_{k,j}),
\end{equation}
where $\hat{c}_{k,j}$ is a candidate cause, $\hat{o}_{k,j}$ is an outcome, and $\hat{r}_{k,j}$ is the causal relation type.
The evidence set $\hat{\mathcal{E}}_k$ maps each tuple to supporting temporal spans or frame references.

\subsection{CERA Design Requirements}
CERA is designed around three joint requirements.

\paragraph{Causal Fidelity.}
Predictions should preserve cause-effect direction and avoid descriptive but non-causal statements.
Operationally, this means the attribution tuples should match reference causal structure, not only lexical overlap.

\paragraph{Evidential Grounding.}
Every attribution claim should be linked to observable evidence in the same event window.
Ungrounded claims are treated as low-trust outputs regardless of language fluency.

\paragraph{Runtime Efficiency.}
Inference must satisfy deployment latency constraints under continuous streams.
Given an end-to-end latency $L_{e2e}$ and deployment budget $L_{max}$, CERA should operate in the feasible region:
\begin{equation}
L_{e2e} \leq L_{max}.
\end{equation}

\subsection{Structured Objective}
We model CERA as maximizing a utility that balances causal quality, evidence quality, and runtime feasibility:
\begin{equation}
\mathcal{U} = \lambda_c S_c + \lambda_e S_e + \lambda_r S_r,
\end{equation}
where $S_c$ measures causal fidelity, $S_e$ measures evidence grounding quality, and $S_r$ measures runtime fitness.

For runtime fitness, we use a normalized score:
\begin{equation}
S_r = \min\left(1,\frac{L_{max}}{L_{e2e}}\right).
\end{equation}
This formulation encourages quality gains only when latency remains practical.

\subsection{Online Inference Contract}
Given an event window $\mathbf{X}_{t_s^k:t_e^k}$, CERA predicts:
\begin{equation}
\hat{\mathbf{y}}_k = \mathcal{F}_{\theta}\!\left(\mathbf{X}_{t_s^k:t_e^k}\right),
\end{equation}
with confidence score $p_k$.
To reduce high-confidence hallucination risk in safety contexts, CERA uses abstention:
\begin{equation}
\hat{\mathbf{y}}_k =
\begin{cases}
\mathcal{F}_{\theta}(\mathbf{X}_{t_s^k:t_e^k}) & \text{if } p_k \ge \tau_{conf},\\
\varnothing & \text{otherwise}.
\end{cases}
\end{equation}

This contract makes failure modes explicit and prevents forcing a causal narrative when evidence is weak.

\subsection{Output Schema and Validation}
For deployment integration, CERA exposes a fixed output schema:
\begin{equation}
\hat{\mathbf{y}}_k = \{\hat{e}_k,\hat{\mathcal{A}}_k,\hat{\mathcal{E}}_k,\hat{s}_k\},
\end{equation}
where $\hat{s}_k$ is a quality status flag (\texttt{valid}, \texttt{abstain}, or \texttt{insufficient\_evidence}).

Before returning \texttt{valid}, CERA applies causal and evidence validators.
For each predicted tuple $\hat{a}_{k,j}=(\hat{c}_{k,j},\hat{r}_{k,j},\hat{o}_{k,j})$, we define:
\begin{equation}
\mathbb{I}^{rel}_{k,j}=\mathbf{1}\!\left[\hat{r}_{k,j}\in\mathcal{R}_{allow}\right],\quad
\mathbb{I}^{dir}_{k,j}=\mathbf{1}\!\left[t(\hat{c}_{k,j})\le t(\hat{o}_{k,j})+\delta_{dir}\right],
\end{equation}
\begin{equation}
\mathbb{I}^{ent}_{k,j}=\mathbf{1}\!\left[\hat{c}_{k,j},\hat{o}_{k,j}\in\mathcal{V}^{track}_{k}\right].
\end{equation}
The causal validator is:
\begin{equation}
\text{CausalCheck}(\hat{\mathcal{A}}_k)=\prod_{j}\left(\mathbb{I}^{rel}_{k,j}\mathbb{I}^{dir}_{k,j}\mathbb{I}^{ent}_{k,j}\right).
\end{equation}

For evidence links $\hat{\mathcal{E}}_{k,j}$ and support scores $q_{k,j}$:
\begin{equation}
\mathbb{I}^{sup}_{k,j}=\mathbf{1}\!\left[q_{k,j}\ge\tau_{evd}\right],\quad
\mathbb{I}^{link}_{k,j}=\mathbf{1}\!\left[|\hat{\mathcal{E}}_{k,j}|\ge1\right],
\end{equation}
with
\begin{equation}
\text{EvidenceCheck}(\hat{\mathcal{A}}_k,\hat{\mathcal{E}}_k)=\prod_{j}\left(\mathbb{I}^{sup}_{k,j}\mathbb{I}^{link}_{k,j}\right).
\end{equation}

Final status is determined by confidence, validation, and runtime budget:
\begin{equation}
\hat{s}_k=
\begin{cases}
\texttt{valid}, & \text{if }\begin{array}[t]{l}
p_k\ge\tau_{conf},\ \text{CausalCheck}=1,\\
\text{EvidenceCheck}=1,\ L_{e2e}^{k}\le L_{max}
\end{array}\\
\texttt{insufficient\_evidence}, & \text{if }\begin{array}[t]{l}
p_k\ge\tau_{conf},\ L_{e2e}^{k}\le L_{max},\\
(\text{CausalCheck}=0\lor \text{EvidenceCheck}=0)
\end{array}\\
\texttt{abstain}, & \text{otherwise}.
\end{cases}
\end{equation}
This policy makes safety behavior explicit: uncertain or ungrounded claims are not promoted to \texttt{valid}.

\subsection{Failure Taxonomy}
To keep failure analysis actionable, we partition output errors into three types:
\begin{itemize}
    \item \textbf{Causal Structure Violation}: cause-effect direction or relation type is wrong.
    \item \textbf{Evidence Support Failure}: claim is plausible but unsupported by linked evidence.
    \item \textbf{Runtime Budget Breach}: quality is acceptable but latency violates deployment budget.
\end{itemize}
This taxonomy maps directly to the three CERA requirements and guides where to improve: reasoning logic, evidence alignment, or system optimization.

\subsection{Module-Level Design}
We decompose CERA into four cooperative modules:
\begin{equation}
\hat{\mathbf{y}}_k =
\mathcal{M}_{ctrl}\!\left(
\mathcal{M}_{evd}\!\left(
\mathcal{M}_{attr}\!\left(
\mathcal{M}_{evt}\!\left(\mathbf{X}_{t_s^k:t_e^k}\right)\right)\right)\right).
\end{equation}
This decomposition keeps responsibilities explicit and aligns each module with one or more CERA requirements.

\paragraph{Event Proposal Module ($\mathcal{M}_{evt}$).}
The event proposal stage generates candidate temporal windows and tracked entities from the stream.
Its goal is high recall under bounded latency, producing a compact event context for downstream reasoning instead of processing the full stream at maximum cost.

\paragraph{Attribution Construction Module ($\mathcal{M}_{attr}$).}
Given event context, CERA builds a directed causal interaction graph
\begin{equation}
\mathcal{G}_k=(\mathcal{V}_k,\mathcal{E}_k^{c}),
\end{equation}
where nodes represent entities/outcomes and directed edges represent candidate causal relations.
Final attribution tuples are decoded from validated graph edges, enforcing direction consistency by construction.

\paragraph{Evidence Alignment Module ($\mathcal{M}_{evd}$).}
For each candidate tuple, CERA links supporting evidence spans in the same event window.
Each claim receives an evidence support score $q_{k,j}$ and is retained only if $q_{k,j}\ge\tau_{evd}$.
This stage prevents fluent but ungrounded attributions from passing as valid outputs.

\paragraph{Budget-Aware Control Module ($\mathcal{M}_{ctrl}$).}
The control module manages compute allocation and output policy under runtime budgets.
It applies adaptive depth, selective decoding, and abstention triggers to keep latency within $L_{max}$ while preserving causal and evidential quality.
When budget pressure is high, the controller prioritizes reliable partial outputs over unstable full attributions.

\subsection{Reference Instantiation (CERA-Ref)}
To make CERA implementation-ready, we define a reference stack (\textbf{CERA-Ref}) with concrete but replaceable components.

\paragraph{Event Backbone.}
CERA-Ref supports two detector backbones with different deployment priorities:
(1) a lightweight one-stage detector (YOLOv8-Nano class) for strict latency budgets, and
(2) a transformer detector (DETR-R50 class) for stronger global scene matching.
These detector families are representative of efficient one-stage and global matching paradigms~\cite{yolo,detr}.
For frame $\mathbf{X}_t$, the detector returns boxes $\mathcal{B}_t$ and risk scores $\mathcal{S}_t$, and triggers downstream reasoning only when the event score exceeds a gate threshold.

\paragraph{Reasoning Backbone.}
For attribution generation, CERA-Ref uses an open-source instruction-tuned 7B VLM with a 336px visual encoder setting.
In our reference profile, this corresponds to a LLaVA-family backbone~\cite{llava,llava15}.
This choice keeps the reasoning backbone reproducible while preserving enough capacity for causal language outputs.

\paragraph{Detection-Guided Token Compaction.}
Before attribution decoding, CERA-Ref applies detection-guided visual token selection:
\begin{equation}
\mathcal{T}^{keep}_t = \mathcal{T}(\mathcal{R}_t)\cup \mathcal{T}\!\left(\text{Dilate}(\mathcal{R}_t,\alpha)\right),
\end{equation}
where $\mathcal{R}_t$ is the detector-derived ROI set and $\alpha$ is a context dilation factor.
This keeps hazard-centric tokens and a thin context ring while pruning background-heavy regions.

\paragraph{Budgeted Decoding.}
During generation, CERA-Ref constrains active KV usage by a budget ratio $\rho_{kv}$:
\begin{equation}
K_{active} = \left\lceil \rho_{kv}\cdot K_{full}\right\rceil, \quad 0<\rho_{kv}\le1.
\end{equation}
The controller selects the top-$K_{active}$ entries via query-conditioned saliency and falls back to abstention or partial report if quality checks fail.

\subsection{Optimization Strategy}
CERA-Ref follows a staged optimization schedule that separates recall, reasoning, and runtime control.

\paragraph{Stage 1: Event Proposal Calibration.}
The detector is tuned with risk-aware weighting:
\begin{equation}
\mathcal{L}_{evt}=\sum_{c}\lambda_c\mathcal{L}^{(c)}_{det},
\end{equation}
where higher-risk classes use larger $\lambda_c$ to preserve recall on safety-critical events.

\paragraph{Stage 2: Attribution and Evidence Learning.}
Given triggered windows, the reasoning module optimizes attribution and evidence objectives jointly:
\begin{equation}
\mathcal{L}_{reason}=\mathcal{L}_{attr}+\gamma\mathcal{L}_{evd}.
\end{equation}
$\mathcal{L}_{attr}$ supervises relation direction/type and tuple content, while $\mathcal{L}_{evd}$ supervises support validity for each tuple.

\paragraph{Stage 3: Budget Controller Tuning.}
Controller parameters $(\tau_{conf},\tau_{evd},\rho_{kv})$ are tuned against deployment constraints by maximizing utility under latency limits:
\begin{equation}
\max \mathcal{U}\quad \text{s.t.}\quad L_{e2e}\le L_{max}.
\end{equation}
This stage determines the operating point between conservative abstention and aggressive throughput.

\subsection{Online Inference Procedure}
At deployment time, CERA-Ref runs the following sequence for each incoming stream window:
\begin{enumerate}
    \item run event proposal and gate non-event windows early,
    \item build attribution candidates from retained event context,
    \item attach evidence spans and compute support scores,
    \item apply schema and consistency checks,
    \item enforce runtime budget policy to output \texttt{valid}, \texttt{insufficient\_evidence}, or \texttt{abstain}.
\end{enumerate}
This pipeline operationalizes CERA as a deterministic contract rather than free-form generation.

\subsection{Current Scope}
This manuscript now defines CERA at task and objective levels with concrete module design.
It also specifies a reference instantiation for implementation.
The next stage is full implementation and empirical validation of each module under shared deployment constraints.

\section{Experiments}

\subsection{Evaluation Targets}
We evaluate CERA along three axes aligned with the method objective:
\begin{itemize}
    \item \textbf{Causal Quality}: correctness of predicted attribution tuples.
    \item \textbf{Evidence Quality}: correctness and completeness of linked evidence references.
    \item \textbf{Runtime}: end-to-end latency and effective throughput under fixed hardware.
\end{itemize}
As an initial benchmark substrate, we plan to start from established surveillance anomaly datasets~\cite{ucfcrime,xdviolence} and extend evaluation with attribution- and evidence-level annotations.

\subsection{Benchmark and Annotation Protocol}
For each video, we derive event windows with fixed temporal stride and event-aware boundary checks.
Each evaluation window is annotated with:
\begin{itemize}
    \item event statement target $e^{*}$,
    \item attribution tuple set $\mathcal{A}^{*}$ with directed relation labels,
    \item evidence span set $\mathcal{E}^{*}$ linking tuples to frame ranges.
\end{itemize}
To reduce annotation drift, each sample is reviewed by two annotators and adjudicated under a fixed causal direction guideline.

\subsection{Baseline Families}
We compare CERA against three baseline groups:
\begin{itemize}
    \item \textbf{Detection-centric}: surveillance anomaly baselines focused on event scoring~\cite{rtfm,mist}.
    \item \textbf{VLM-adapted anomaly systems}: models that couple language outputs with anomaly detection~\cite{vadclip,anomalygpt,holmes_vad}.
    \item \textbf{Video-language reasoning}: general long-video reasoning models~\cite{video_chatgpt,videollava}.
    We additionally include long-context variants~\cite{timechat,moviechat}.
\end{itemize}
Within CERA, we report profile variants (YOLO-class and DETR-class detectors) under identical downstream attribution/evidence modules.

\subsection{Metric Definitions}
Let $\hat{\mathcal{A}}$ and $\mathcal{A}^{*}$ be predicted and reference attribution tuples.
Tuple matching requires aligned cause, outcome, and relation direction.
With matched pair set $\mathcal{M}_{A}$:
\begin{equation}
P_A=\frac{|\mathcal{M}_{A}|}{|\hat{\mathcal{A}}|},\quad
R_A=\frac{|\mathcal{M}_{A}|}{|\mathcal{A}^{*}|},\quad
F1_A=\frac{2P_AR_A}{P_A+R_A}.
\end{equation}

For evidence, let $\hat{\mathcal{E}}$ and $\mathcal{E}^{*}$ denote predicted and reference support links.
We compute support precision/recall/F1 analogously as $P_E,R_E,F1_E$.
Runtime is reported by latency, throughput, and budget-feasibility rate:
\begin{equation}
R_{feas}=\frac{1}{K}\sum_{k=1}^{K}\mathbf{1}\!\left[L_{e2e}^{k}\le L_{max}\right].
\end{equation}
To evaluate status-policy behavior, we also report:
\begin{equation}
R_{valid}=\frac{1}{K}\sum_{k=1}^{K}\mathbf{1}\!\left[\hat{s}_{k}=\texttt{valid}\right],
\end{equation}
\begin{equation}
R_{unsafe}=\frac{\sum_{k=1}^{K}\mathbf{1}\!\left[\hat{s}_{k}=\texttt{valid}\land (\text{TupleMatch}_{k}=0\lor \text{EvidenceMatch}_{k}=0)\right]}{\sum_{k=1}^{K}\mathbf{1}\!\left[\hat{s}_{k}=\texttt{valid}\right]+\epsilon}.
\end{equation}
Lower $R_{unsafe}$ indicates safer deployment-time behavior under the same coverage.
We additionally report mean utility $\bar{\mathcal{U}}$ to summarize quality--latency trade-offs in one score.

\subsection{Protocol Principles}
To keep comparisons meaningful, all compared systems will use the same runtime protocol:
hardware, input resolution, decoding length, and reporting policy.
Final claims will be tied to both quality metrics and latency feasibility under deployment budgets.
Detector-swap comparisons (YOLO-class vs DETR-class) will keep the rest of the CERA pipeline fixed.

\subsection{Implementation and Reproducibility}
All reference settings are versioned in CERA configs.
We use `base.yaml` for the YOLO profile and `base\_detr.yaml` for the DETR profile.
The default profile uses YOLO-class event proposal and an open 7B VLM backbone; the DETR profile swaps only detector-related parameters and adjusted latency budget.
In the default reference profile, key control parameters are initialized as $\tau_{conf}=0.45$, $\tau_{evd}=0.55$, $\rho_{kv}=0.25$, and $L_{max}=120$ms.
For reproducibility, we fix random seeds, report configuration files used per run, and release per-video prediction dumps for attribution and evidence outputs.

\subsection{Statistical Reporting}
Main results will be reported over multiple random seeds with mean and standard deviation.
We will attach 95\% bootstrap confidence intervals for $F1_A$, $F1_E$, and $R_{feas}$, and perform paired significance testing on per-video metrics.
This is intended to separate stable gains from run-to-run variation.

\subsection{Planned Ablations}
To validate each design choice, we plan a component-wise ablation suite:
\begin{itemize}
    \item \textbf{Full CERA-Ref}: event proposal + attribution + evidence alignment + budget controller.
    \item \textbf{Detector Backbone Swap}: YOLO-class vs DETR-class under identical downstream settings.
    \item \textbf{No Event Gating}: process all windows to measure temporal-efficiency contribution.
    \item \textbf{No Token Compaction}: disable detection-guided token selection.
    \item \textbf{No Budgeted Decoding}: use dense decoding to isolate controller impact.
    \item \textbf{No Evidence Gate}: keep attribution without evidence-threshold filtering.
\end{itemize}
We will report both metric deltas and utility changes to reveal where quality--latency trade-offs originate.

\section{Conclusion}
This paper established \textbf{CERA} as a practical objective for real-time surveillance understanding: produce causal event reports that are structurally correct, evidentially grounded, and operationally feasible.
We formalized this objective through (1) a structured task definition, (2) a utility that balances causal quality, grounding quality, and runtime fitness, (3) an online contract with abstention and explicit status outputs, and (4) a concrete module-level design for event proposal, attribution construction, evidence alignment, and budget-aware control.
We also defined an initial evaluation protocol aligned with these requirements.

The next validation stage is empirical.
We plan to execute CERA-Ref end to end, run staged ablations, build annotated benchmarks for attribution and evidence linkage, and compare against detection- and caption-centric baselines under matched runtime settings.
This progression is intended to test whether causal reasoning quality can improve without violating deployment latency budgets.

\bibliographystyle{splncs04}
\bibliography{main}

\end{document}
