\documentclass[runningheads]{llncs}

% ---------------------------------------------------------------
% Include basic ECCV package
 
% TODO REVIEW: Insert your submission number below by replacing '*****'
% TODO FINAL: Comment out the following line for the camera-ready version
\usepackage[review,year=2024,ID=*****]{eccv}
% TODO FINAL: Un-comment the following line for the camera-ready version
%\usepackage{eccv}

% ---------------------------------------------------------------
% Other packages
\usepackage{eccvabbrv}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage[section]{placeins}
\usepackage{float}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[accsupp]{axessibility}

% ---------------------------------------------------------------
% Hyperref package
% TODO FINAL: Comment out the following line for the camera-ready version
\usepackage[pagebackref,breaklinks,colorlinks,citecolor=eccvblue]{hyperref}
% TODO FINAL: Un-comment the following line for the camera-ready version
%\usepackage{hyperref}

\usepackage{orcidlink}

\newcommand{\Idom}{\mathcal{I}_{\text{dom}}}
\newcommand{\Ttop}{\mathcal{T}}


\begin{document}

% ---------------------------------------------------------------
\title{Event-VLM: Three-Axis Efficient Inference for\\Real-time Surveillance Video Understanding}

\titlerunning{Event-VLM}

\author{First Author\inst{1}\orcidlink{0000-1111-2222-3333} \and
Second Author\inst{2,3}\orcidlink{1111-2222-3333-4444} \and
Third Author\inst{3}\orcidlink{2222--3333-4444-5555}}

\authorrunning{F.~Author et al.}

\institute{Princeton University, Princeton NJ 08544, USA \and
Springer Heidelberg, Tiergartenstr.~17, 69121 Heidelberg, Germany
\email{lncs@springer.com}\\
\url{http://www.springer.com/gp/computer-science/lncs} \and
ABC Institute, Rupert-Karls-University Heidelberg, Heidelberg, Germany\\
\email{\{abc,lncs\}@uni-heidelberg.de}}

\maketitle


% --- ABSTRACT ---
\begin{abstract}
Vision-Language Models (VLMs) are attractive for intelligent surveillance, but their deployment is limited by inference economics: processing hundreds of concurrent streams with full-frame, full-cache decoding is prohibitively expensive. We show that this cost is driven by three orthogonal redundancies: \textit{temporal} (most frames are non-events), \textit{spatial} (most visual tokens are irrelevant background), and \textit{decoding} (auto-regressive KV-cache access becomes memory-bandwidth bound). We present \textbf{Event-VLM}, a cascaded framework that allocates computation only \textit{when} events occur, \textit{where} hazards reside, and \textit{which} cache entries matter during generation. Event-VLM combines: (1) \textit{Event-Triggered Gating} with risk-sensitive detection loss for high recall on critical hazards; (2) \textit{Knowledge-Guided Token Pruning} with adaptive dilation for amorphous hazards, reducing visual tokens by 75\% without retraining; and (3) \textit{Frequency-Aware Sparse Decoding}, which uses dominant RoPE frequency chunks as a training-free proxy for online token importance and focused KV access. An optional hazard-priority prompting module further improves domain-specific explanation quality with negligible overhead. On UCF-Crime and XD-Violence, Event-VLM achieves \textbf{9$\times$} higher throughput than strong VLM baselines while preserving 99\% caption quality, providing a practical blueprint for real-time, large-scale safety monitoring.

\keywords{Vision-Language Models, Efficient Inference, Surveillance, Token Pruning, KV Cache Optimization, Sparse Attention}
\end{abstract}


% --- INTRODUCTION ---
\section{Introduction}

\paragraph{The Paradigm Shift.}
Large Vision-Language Models (VLMs) such as LLaVA~\cite{llava} and GPT-4V~\cite{gpt4v} have revolutionized visual understanding, enabling systems to move beyond simple object detection toward comprehensive \textit{accident explanation}---describing not only what happened but \textit{why} and \textit{how} an event unfolded. In Intelligent Surveillance Systems (ISS), this capability is critical: understanding that ``a worker collapsed due to being struck by a falling object'' is far more actionable than simply reporting ``a person detected.''

\paragraph{The Scalability Bottleneck.}
Deploying VLMs in real-world surveillance presents a fundamental \textbf{scalability bottleneck}. Unlike offline video analysis, surveillance systems must process continuous, high-resolution streams from hundreds of concurrent cameras. We identify three orthogonal axes of computational redundancy that make this prohibitively expensive (Fig.~\ref{fig:framework}):

\begin{itemize}
    \item \textbf{Temporal Redundancy}: In typical surveillance footage, critical events occupy less than 1\% of the total duration. Processing every frame with a heavy VLM is wasteful.
    \item \textbf{Spatial Redundancy}: In a typical CCTV view, the vast majority of visual tokens (walls, sky, empty roads) are semantically irrelevant to any safety event. Feeding these tokens into costly self-attention layers is a significant waste.
    \item \textbf{Decoding Redundancy}: During auto-regressive text generation, the Key-Value (KV) cache grows linearly with context length. At each decoding step, the model must access the \textit{entire} cache, creating a memory-bandwidth bottleneck that underutilizes modern GPUs~\cite{flash_attention}. This is particularly acute in VLMs, where hundreds of visual tokens inflate the KV cache.
\end{itemize}

\paragraph{Limitations of Existing Work.}
Existing acceleration methods usually optimize one axis while leaving the others as hidden bottlenecks. Temporal methods (e.g., SeViLA~\cite{sevila}) reduce frame count but still process dense spatial tokens and full decoding state. Spatial pruning methods (ToMe~\cite{tome}, DynamicViT~\cite{dynamicvit}) shrink token sets but are often domain-agnostic, risking the removal of small yet safety-critical cues. KV cache methods (StreamingLLM~\cite{streamingllm}, SnapKV~\cite{snapkv}) target language-only settings and are not designed for visual-textual surveillance contexts. As a result, single-axis acceleration quickly saturates in end-to-end deployment.

\paragraph{Our Approach: Event-VLM.}
We propose \textbf{Event-VLM}, a cascaded framework built on a simple systems principle: allocate computation only \textit{when} an event occurs, \textit{where} the hazard is located, and \textit{which} memory entries are relevant for generation.
\begin{enumerate}
    \item \textbf{Event-Triggered Gating} (Temporal) uses a lightweight detector with \textit{risk-sensitive loss} to suppress background frames while preserving recall on critical hazards.
    \item \textbf{Knowledge-Guided Token Pruning} (Spatial) converts detector priors into dynamic token masks, with \textit{adaptive dilation} for amorphous hazards such as fire and smoke.
    \item \textbf{Frequency-Aware Sparse Decoding} (Decoding) adapts RoPE frequency-chunk functional sparsity~\cite{fasa} to VLM decoding, enabling training-free token importance estimation and focused KV access during generation.
\end{enumerate}

These three stages are \textbf{training-free} with respect to the VLM backbone, requiring no backbone modification or full-model fine-tuning. We additionally include an optional, lightweight prompting module for domain adaptation.

\paragraph{Contributions.}
\begin{itemize}
    \item We present \textbf{Event-VLM}, the first surveillance-oriented VLM framework that jointly optimizes temporal, spatial, and decoding efficiency in a single end-to-end pipeline.
    \item We introduce \textbf{risk-sensitive gating} and \textbf{adaptive spatial pruning}, which explicitly encode hazard severity and object morphology to preserve critical evidence under aggressive compute budgets.
    \item We adapt \textbf{frequency-aware sparse decoding} to mixed visual-textual generation, showing that dominant RoPE frequency chunks are an effective training-free proxy for KV saliency.
    \item On UCF-Crime and XD-Violence, Event-VLM delivers \textbf{9$\times$} higher throughput while retaining 99\% caption quality, establishing a practical path to real-time multi-camera deployment.
\end{itemize}


% --- RELATED WORK ---
\section{Related Work}

\subsection{Large Vision-Language Models}
The convergence of vision and language has produced VLMs capable of complex multimodal reasoning. CLIP~\cite{clip} pioneered visual-textual alignment via contrastive learning. Generative models such as Flamingo~\cite{flamingo} and BLIP-2~\cite{blip2} bridged frozen encoders with LLMs, while the LLaVA family~\cite{llava,llava15,llavanext} showed that simple projection architectures achieve remarkable visual instruction following. Qwen-VL~\cite{qwenvl} and CogVLM~\cite{cogvlm} introduced visual experts and grounding capabilities.

For video understanding, Video-LLaMA~\cite{videollama} and VideoChat~\cite{videochat} extended image VLMs with temporal modeling, while VILA~\cite{vila} showed that video pre-training improves temporal reasoning. However, these models employ heavy visual encoders with billion-parameter LLMs, and the quadratic attention complexity~\cite{attention} makes real-time deployment challenging.

\subsection{Efficient Vision Transformers and Token Pruning}
DynamicViT~\cite{dynamicvit} introduced learnable modules for progressive token discard. EViT~\cite{evit} fused less attentive tokens into representative tokens. ToMe~\cite{tome} proposed training-free token merging based on key-value similarity, achieving 2$\times$ speedup. SPViT~\cite{spvit} optimized pruning for latency. For video, SeViLA~\cite{sevila} employed keyframe selection and SlowFast~\cite{slowfast} proposed dual-pathway architectures.

Despite their effectiveness, existing pruning methods rely on statistical importance without domain knowledge. In safety-critical scenarios, small hazards may have low statistical prominence and risk being pruned. Our \textbf{Knowledge-Guided Token Pruning} addresses this by leveraging detection priors~\cite{yolo}.

\subsection{Vision-Language Models for Anomaly Detection}
Traditional Video Anomaly Detection (VAD) relied on reconstruction errors~\cite{ucfcrime,vad_survey} or temporal features~\cite{rtfm,i3d}. Recently, VLMs enabled explainable detection: AnomalyGPT~\cite{anomalygpt} fine-tuned VLMs on defect datasets, Holmes-VAD~\cite{holmes_vad} proposed multi-modal detection with chain-of-thought reasoning, and VADCLIP~\cite{vadclip} adapted CLIP for weakly-supervised VAD. However, these assume offline single-stream processing.

\subsection{KV Cache Optimization and Sparse Attention}
The growing KV cache is a critical bottleneck for LLM inference. StreamingLLM~\cite{streamingllm} maintained a fixed-size window with ``attention sinks.'' H2O~\cite{h2o} evicted tokens based on cumulative attention. SnapKV~\cite{snapkv} selected important KV entries per layer. PyramidKV~\cite{pyramidkv} allocated varying budgets across layers. However, these methods use heuristic importance measures that are not truly query-aware.

FASA~\cite{fasa} recently discovered that RoPE~\cite{rope} induces \textit{functional sparsity} at the frequency-chunk level: a small subset of ``dominant'' frequency chunks captures contextual attention patterns, while the majority encode positional structures. This provides a training-free, query-aware proxy for token importance. While FASA targets text-only LLMs, we adapt this insight to the VLM setting, where visual tokens significantly inflate the KV cache.

\subsection{Positioning of Event-VLM}
Table~\ref{tab:method_comparison} summarizes our positioning. Event-VLM is the first to address temporal, spatial, \textit{and} decoding efficiency simultaneously, all in a training-free manner with domain-aware optimization.

\begin{table}[t]
\caption{\textbf{Comparison of Efficiency Strategies.} Event-VLM uniquely addresses all three axes of redundancy with training-free, domain-aware optimization.}
\label{tab:method_comparison}
\centering
\setlength{\tabcolsep}{3pt}
\begin{tabular}{l|cccccc}
\toprule
\textbf{Method} & \textbf{Venue} & \textbf{Temporal} & \textbf{Spatial} & \textbf{Decoding} & \textbf{Train-free} & \textbf{Domain} \\
\midrule
DynamicViT~\cite{dynamicvit} & NeurIPS'21 & - & \checkmark & - & - & - \\
ToMe~\cite{tome} & ICLR'23 & - & \checkmark & - & \checkmark & - \\
SeViLA~\cite{sevila} & NeurIPS'23 & \checkmark & - & - & - & - \\
StreamingLLM~\cite{streamingllm} & ICLR'24 & - & - & \checkmark & \checkmark & - \\
SnapKV~\cite{snapkv} & NeurIPS'24 & - & - & \checkmark & \checkmark & - \\
FASA~\cite{fasa} & arXiv'26 & - & - & \checkmark & \checkmark & - \\
Holmes-VAD~\cite{holmes_vad} & CVPR'25 & - & - & - & - & \checkmark \\
\midrule
\textbf{Event-VLM} & - & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark \\
\bottomrule
\end{tabular}
\end{table}


% --- METHOD ---
\section{Method}

\begin{figure}[!t]
  \centering
  \includegraphics[width=0.95\textwidth,height=0.32\textheight,keepaspectratio]{figures/figure1_architecture.png}
  \caption{\textbf{Overview of the Event-VLM Framework.}
  Our system eliminates redundancy along three orthogonal axes:
  (1) \textbf{Temporal}: Event-Triggered Gating filters background frames.
  (2) \textbf{Spatial}: Knowledge-Guided Token Pruning removes irrelevant visual tokens.
  (3) \textbf{Decoding}: Frequency-Aware Sparse Decoding optimizes KV cache access during generation.
  (4) \textbf{Adaptation}: Hazard-Priority Prompting tailors the VLM to safety-critical reasoning.}
  \label{fig:framework}
\end{figure}

\subsection{Overview}
Our goal is to process continuous surveillance streams $\mathcal{V} = \{\mathbf{X}_1, \mathbf{X}_2, \dots\}$ in real-time while generating accurate accident descriptions $\mathbf{Y}$.
As illustrated in Fig.~\ref{fig:framework}, \textbf{Event-VLM} operates as a cascaded pipeline addressing three axes of redundancy:
\begin{enumerate}
    \item \textit{Event-Triggered Gating} ($\mathcal{F}_{gate}$) filters out background frames (\textbf{temporal});
    \item \textit{Knowledge-Guided Token Pruning} ($\mathcal{F}_{prune}$) reduces visual tokens via detection priors (\textbf{spatial});
    \item \textit{Frequency-Aware Sparse Decoding} ($\mathcal{F}_{sparse}$) optimizes KV cache access during generation (\textbf{decoding});
    \item \textit{Hazard-Priority Prompting} ($\mathcal{P}_{ctx}$) adapts VLM reasoning to safety domains (\textbf{adaptation}).
\end{enumerate}
The overall inference for a frame $\mathbf{X}_t$ is:
\begin{equation}
    \mathbf{Y}_t = \mathcal{F}_{sparse}\!\left(\mathcal{F}_{gen}\!\left(\mathcal{F}_{prune}(\mathbf{X}_t, \mathcal{B}_t) \mid \mathcal{P}_{ctx}\right)\right) \quad \text{if} \quad \mathcal{F}_{gate}(\mathbf{X}_t) = 1,
\end{equation}
where $\mathcal{B}_t$ are detected bounding boxes and $\mathcal{P}_{ctx}$ are learnable context prompts.


\subsection{Stage 1: Event-Triggered Gating (Temporal Axis)}
Processing every frame with a VLM is computationally redundant. We use a lightweight detector (YOLOv8-Nano) as a \textit{Trigger Module}.
For frame $\mathbf{X}_t$, the detector predicts bounding boxes $\mathcal{B}_t = \{b_1, \dots, b_N\}$ with class scores $\mathcal{S}_t = \{s_1, \dots, s_N\}$. The binary gating function is:
\begin{equation}
    \mathbb{I}_{event}(\mathbf{X}_t) = 
    \begin{cases} 
    1 & \text{if } \exists k,\; s_k > \tau_{conf} \text{ and } c_k \in \mathcal{C}_{hazard} \\
    0 & \text{otherwise}.
    \end{cases}
\end{equation}

\subsubsection{Risk-Sensitive Detection Loss.}
To prevent error propagation in our cascaded design, we propose a \textit{Risk-Sensitive Detection Loss} that prioritizes high-risk categories.
We partition $\mathcal{C}_{hazard}$ into severity tiers: $\mathcal{C}_{critical}$ (fire, smoke, collapse), $\mathcal{C}_{high}$ (forklift, heavy machinery), $\mathcal{C}_{standard}$ (person, vehicle):
\begin{equation}
    \mathcal{L}_{detect} = \sum_{k=1}^{N} w(c_k) \cdot \mathcal{L}_{focal}(p_k, y_k), \quad
    w(c_k) = \begin{cases}
    \lambda_{crit} & \text{if } c_k \in \mathcal{C}_{critical} \\
    \lambda_{high} & \text{if } c_k \in \mathcal{C}_{high} \\
    1.0 & \text{otherwise}.
    \end{cases}
\end{equation}
By setting $\lambda_{crit} > \lambda_{high} > 1$, we bias the detector toward higher recall on life-threatening events.


\subsection{Stage 2: Knowledge-Guided Token Pruning (Spatial Axis)}
Standard VLMs process all patch tokens regardless of semantic density. We prune background tokens using detection priors from Stage~1 in a training-free manner.

\subsubsection{Dynamic Mask Generation.}
Let the ViT divide frame $\mathbf{X}_t \in \mathbb{R}^{H \times W \times 3}$ into $L = (H/P) \times (W/P)$ patch tokens $\mathbf{Z} = \{z_1, \dots, z_L\}$.
We map each bounding box to patch grid indices and define a binary importance mask $\mathbf{M} \in \{0,1\}^L$:
\begin{equation}
    \mathbf{M}_i = \mathbb{I}\left( i \in \bigcup_{k} \Omega(b_k) \right),
\end{equation}
where $\Omega(b_k)$ is the set of patch indices covered by the (dilated) box $b_k$.

\subsubsection{Adaptive Dilation for Amorphous Objects.}
Amorphous hazards (fire, smoke) have high intraclass shape variance. We adjust the bounding box expansion factor per class:
\begin{equation}
    \alpha_k = \alpha_{base} \cdot \left(1 + \beta \cdot \sigma_{shape}(c_k)\right),
\end{equation}
where $\sigma_{shape}(c_k)$ is the normalized intraclass shape variance precomputed from training data.

\subsubsection{Pruning Operation.}
The reduced token sequence is:
\begin{equation}
    \hat{\mathbf{Z}} = \{ z_i \mid \mathbf{M}_i = 1 \} \cup \{ z_{cls} \},
\end{equation}
with length $L' \ll L$, reducing self-attention complexity from $\mathcal{O}(L^2)$ to $\mathcal{O}(L'^2)$.


\subsection{Stage 3: Frequency-Aware Sparse Decoding (Decoding Axis)}
\label{sec:fasa_decoding}

While Stages 1--2 reduce \textit{input-side} compute, auto-regressive generation can still dominate latency because each step reads a growing KV cache that includes visual tokens. We therefore introduce \textbf{Frequency-Aware Sparse Decoding}, a two-step strategy that adapts RoPE functional sparsity~\cite{fasa} to VLM decoding: (i) \textit{Token Importance Prediction} (TIP) in a compact frequency subspace, followed by (ii) \textit{Focused Attention Computation} (FAC) on a selected token subset.

\subsubsection{Background: Functional Sparsity in RoPE.}
In RoPE-based models~\cite{rope} (e.g., LLaMA/Vicuna used in LLaVA), each $d$-dimensional query/key vector is partitioned into $d/2$ orthogonal 2D subspaces called \textit{frequency chunks} (FCs). Each FC $i$ rotates at angular frequency $\theta_i = B^{-2(i-1)/d}$.

A recent discovery~\cite{fasa} reveals that these FCs exhibit \textit{functional sparsity}: they can be categorized into two groups:
\begin{itemize}
    \item \textbf{Contextual FCs}: A small subset responsible for dynamic, query-dependent attention---identifying which tokens are semantically relevant.
    \item \textbf{Structural FCs}: The remaining majority that encode fixed positional patterns (recency bias, attention sinks).
\end{itemize}
Critically, the set of contextual FCs (termed \textit{dominant FCs}, $\Idom$) is \textbf{sparse}, \textbf{universal across tasks}, and identifiable via a one-time offline calibration.

\subsubsection{Offline Calibration of Dominant FCs.}
We perform a one-time calibration to identify $\Idom^{l,h}$ for each attention head $(l,h)$ in the LLM backbone. Using a small calibration set $\Omega$ and the Contextual Agreement (CA) metric~\cite{fasa}, we measure how well each single FC's attention pattern aligns with the full head:
\begin{equation}
    \text{CA}_{\mathcal{K}}^{l,h,i} = \frac{|\text{TopK}(\boldsymbol{\alpha}_{l,h}, \mathcal{K}) \cap \text{TopK}(\boldsymbol{\alpha}_{l,h}^{(i)}, \mathcal{K})|}{\mathcal{K}},
\end{equation}
where $\boldsymbol{\alpha}_{l,h}$ is the full attention score vector and $\boldsymbol{\alpha}_{l,h}^{(i)}$ uses only the $i$-th FC.
The dominant set is selected as:
\begin{equation}
    \Idom^{l,h} = \text{TopK-I}\!\left(\{\text{CA}_{\mathcal{K}}^{l,h,i}\}_{i=0}^{d/2-1},\; F\right),
\end{equation}
where $F \ll d/2$ is the number of dominant FCs (typically $F \approx d/8$).

\subsubsection{Online Token Importance Prediction.}
During decoding at step $t$, instead of computing full attention over the entire KV cache, we estimate token importance using only the dominant FCs:
\begin{equation}
    \mathbf{S}_t^{l,h} = \sum_{i \in \Idom^{l,h}} \boldsymbol{\alpha}^{l,h,i}(\mathbf{q}_t, \mathbf{K}_{1:t}),
\end{equation}
where $\boldsymbol{\alpha}^{l,h,i}$ denotes the partial attention score computed from the $i$-th FC subspace only. This operates in a greatly reduced dimensionality ($2F$ vs.\ $d$), making the importance estimation computationally frugal.

\subsubsection{Focused Attention Computation.}
Based on the importance scores, we select the top-$N_{fac}$ most important tokens:
\begin{equation}
    \Ttop_t = \text{TopK-I}(\mathbf{S}_t^{l,h},\; N_{fac}).
\end{equation}
Full-fidelity attention is then computed \textit{only} on this salient subset:
\begin{equation}
    \hat{\mathbf{K}}_t = \text{Gather}(\mathbf{K}_{1:t}, \Ttop_t), \quad \hat{\mathbf{V}}_t = \text{Gather}(\mathbf{V}_{1:t}, \Ttop_t),
\end{equation}
\begin{equation}
    \mathbf{o}_t^{l,h} = \text{softmax}\!\left(\frac{\mathbf{q}_t \hat{\mathbf{K}}_t^\top}{\sqrt{d}}\right) \hat{\mathbf{V}}_t.
\end{equation}
The original absolute positions of tokens in $\Ttop_t$ are preserved, maintaining the integrity of RoPE positional encodings.

\subsubsection{Complexity Analysis.}
Standard decoding attention has complexity $\mathcal{O}(td)$ per head. Our approach requires $\mathcal{O}(2tF)$ for TIP (importance prediction in the $F$-dimensional subspace) and $\mathcal{O}(N_{fac}d)$ for FAC (focused attention on the reduced set). The total complexity is $\mathcal{O}(2tF + N_{fac}d)$, yielding a theoretical speedup of:
\begin{equation}
    \text{Speedup} = \frac{2td}{2tF + N_{fac}d} \approx \frac{d}{F} \quad \text{when } N_{fac} \ll t.
\end{equation}
With $F = d/8$, this provides up to $8\times$ reduction in memory bandwidth during the decoding phase.


\subsection{Stage 4: Context-Aware Prompt Tuning (Adaptation)}
Beyond efficiency, we include an optional adaptation module for domain-specific explanation quality. We employ \textit{Soft Prompt Tuning} with learnable vectors $\mathcal{P}_{ctx} \in \mathbb{R}^{K \times D}$ prepended to text embeddings:
\begin{equation}
    \mathcal{L} = - \sum_{j=1}^{|\mathbf{Y}|} \log P_{\theta}(y_j \mid y_{<j}, \hat{\mathbf{Z}}, \mathcal{P}_{ctx}),
\end{equation}
where $\theta$ are frozen VLM parameters. Only $\mathcal{P}_{ctx}$ is updated ($<$0.1\% parameters).

\subsubsection{Hazard-Priority Prompting.}
Different hazards require different reasoning granularity. We dynamically select from a hierarchical prompt bank:
\begin{equation}
    \mathcal{P}_{active} = \begin{cases}
    \mathcal{P}_{critical} & \text{if } \max_k w(c_k) \geq \lambda_{crit} \\
    \mathcal{P}_{standard} & \text{otherwise},
    \end{cases}
\end{equation}
where $\mathcal{P}_{critical}$ contains specialized safety prompts and $\mathcal{P}_{standard}$ contains general prompts.


% --- EXPERIMENTS ---
\section{Experiments}

\subsection{Experimental Setup}

\subsubsection{Datasets.}
We evaluate on two large-scale video anomaly detection datasets:
\begin{itemize}
    \item \textbf{UCF-Crime}~\cite{ucfcrime}: 1,900 real-world surveillance videos covering 13 anomaly types.
    \item \textbf{XD-Violence}~\cite{xdviolence}: A multi-modal dataset with violent events; we use the video modality.
\end{itemize}
We enriched a subset ($\sim$500 clips) with manual dense captions for accident explanation evaluation, following~\cite{anomalygpt}.

\subsubsection{Implementation Details.}
We use \textbf{YOLOv8-Nano} ($\sim$1ms/frame) as the trigger and frozen \textbf{LLaVA-1.5-7B}~\cite{llava} (CLIP-ViT-L/14-336px + Vicuna-7B) as the VLM backbone. The Vicuna-7B LLM uses RoPE, enabling our frequency-aware sparse decoding.
For Stage~3, we calibrate dominant FCs using 32 samples from the training set with $F=16$ (25\% of $d/2=64$ FCs) and $N_{fac}=256$ tokens. The confidence threshold is $\tau_{conf}=0.5$ and dilation base is $\alpha_{base}=1.2$.
All experiments are conducted on a single \textbf{NVIDIA RTX 5080 GPU}.


\subsection{Comparison with State-of-the-Arts}

\begin{table}[t]
\caption{\textbf{Main Results on UCF-Crime.} Event-VLM achieves a superior trade-off across all three efficiency axes. Speed measured on RTX 5080.}
\label{tab:main_results}
\centering
\setlength{\tabcolsep}{4pt}
\begin{tabular}{l|c|cc|cc}
\toprule
\textbf{Model} & \textbf{Method} & \textbf{AUC (\%)} & \textbf{CIDEr} & \textbf{GFLOPs} $\downarrow$ & \textbf{FPS} $\uparrow$ \\
\midrule
\textit{Traditional VAD} & & & & & \\
Sultani et al.~\cite{ucfcrime} & C3D & 75.4 & - & 0.8 & \textbf{320} \\
RTFM~\cite{rtfm} & I3D & 84.3 & - & 2.1 & 145 \\
\midrule
\textit{Large VLMs} & & & & & \\
Video-LLaMA~\cite{videollama} & Full Frame & 81.5 & 82.3 & 450.2 & 3.5 \\
LLaVA-1.5~\cite{llava} & Frame-by-Frame & 85.0 & \textbf{90.1} & 180.5 & 5.2 \\
\midrule
\textit{Efficient VLMs} & & & & & \\
SeViLA~\cite{sevila} & Keyframe Sel. & 84.5 & 88.0 & 108.3 & 12.0 \\
LLaVA + ToMe~\cite{tome} & Statistical Pruning & 82.1 & 85.4 & 90.2 & 15.6 \\
LLaVA + SnapKV~\cite{snapkv} & KV Eviction & 84.2 & 87.8 & 95.0 & 14.2 \\
\midrule
\textbf{Event-VLM (Ours)} & \textbf{3-Axis Optim.} & \textbf{84.8} & \textbf{89.5} & \textbf{45.1} & \textbf{48.2} \\
\bottomrule
\end{tabular}
\end{table}

As shown in Table~\ref{tab:main_results}, traditional VAD methods are fast but lack interpretability. Heavy VLMs achieve high caption quality but suffer from low throughput ($<$6 FPS). Methods addressing a single axis (SeViLA: temporal; ToMe: spatial; SnapKV: decoding) provide moderate improvements. \textbf{Event-VLM} maintains 99\% of caption quality (89.5 vs.\ 90.1 CIDEr) while running \textbf{9$\times$} faster (48.2 FPS), demonstrating the synergistic benefit of addressing all three axes.


\subsection{Ablation Studies}

\begin{table}[t]
\caption{\textbf{Three-Axis Ablation.} Sequential activation of each efficiency axis on top of the LLaVA-1.5 baseline.}
\label{tab:ablation_component}
\centering
\setlength{\tabcolsep}{3pt}
\begin{tabular}{cccc|ccc}
\toprule
\textbf{Temporal} & \textbf{Spatial} & \textbf{Decoding} & \textbf{Prompt} & \textbf{FPS} $\uparrow$ & \textbf{AUC} & \textbf{CIDEr} \\
\midrule
- & - & - & - & 5.2 & 85.0 & 90.1 \\
\checkmark & - & - & - & 18.5 & 85.0 & 90.1 \\
\checkmark & \checkmark & - & - & 38.5 & 84.8 & 89.2 \\
\checkmark & \checkmark & \checkmark & - & \textbf{48.2} & 84.8 & 89.0 \\
\checkmark & \checkmark & \checkmark & \checkmark & 48.0 & \textbf{85.6} & \textbf{89.5} \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{Three-Axis Component Analysis.}
Table~\ref{tab:ablation_component} demonstrates the contribution of each axis. The temporal axis (Event Trigger) provides the largest speedup by skipping background frames. The spatial axis (Token Pruning) further boosts FPS from 18.5 to 38.5. The decoding axis (Frequency-Aware Sparse Decoding) provides an additional 25\% speedup (38.5 $\to$ 48.2 FPS) by reducing KV cache memory bandwidth during generation. The prompt tuning slightly improves quality without affecting speed.

\subsubsection{Frequency-Aware Decoding Analysis.}

\begin{table}[t]
\caption{\textbf{Sparse Decoding Comparison.} Performance of different KV cache strategies applied to the LLM backbone during VLM inference.}
\label{tab:decoding_comparison}
\centering
\setlength{\tabcolsep}{4pt}
\begin{tabular}{l|ccc}
\toprule
\textbf{Decoding Strategy} & \textbf{CIDEr} & \textbf{Decoding Speedup} & \textbf{Training-free} \\
\midrule
Full KV Cache & 90.1 & 1.0$\times$ & - \\
StreamingLLM~\cite{streamingllm} & 84.2 & 1.8$\times$ & \checkmark \\
H2O~\cite{h2o} & 86.5 & 1.6$\times$ & \checkmark \\
SnapKV~\cite{snapkv} & 87.8 & 1.7$\times$ & \checkmark \\
\textbf{FC-Sparse (Ours)} & \textbf{89.0} & \textbf{2.1$\times$} & \checkmark \\
\bottomrule
\end{tabular}
\end{table}

Table~\ref{tab:decoding_comparison} compares our frequency-aware sparse decoding against existing KV cache strategies. StreamingLLM's fixed-window approach loses critical visual context. H2O and SnapKV use heuristic importance and underperform for safety-critical captions. Our FC-based approach, being truly query-aware, preserves the most relevant visual and textual tokens, achieving the best quality-speed trade-off.

\subsubsection{Dominant FC Analysis in VLM Context.}
A key question is whether the functional sparsity discovered in text-only LLMs~\cite{fasa} transfers to the VLM setting, where the KV cache contains both visual and textual tokens. We compute CA scores across the Vicuna-7B backbone under mixed visual-textual inputs and observe: (1) functional sparsity persists, with a small FC subset dominating contextual attention; (2) dominant FC identities remain largely consistent between text-only and visual-textual regimes; and (3) with $F=16$ FCs (25\%), CA remains above 0.85 across layers, supporting reliable token-importance prediction.

\subsubsection{Pruning Robustness.}
Our knowledge-guided pruning maintains robust performance even at 20\% token retention, whereas ToMe suffers a sharp drop after 50\% reduction. This confirms that domain-aware pruning (\textit{where} to prune) matters more than statistical pruning (\textit{how much}).

\subsubsection{Trigger Reliability.}
The YOLOv8-Nano trigger achieves \textbf{98.2\%} recall on hazard frames with $\tau_{conf}=0.5$, with missed cases primarily due to extreme occlusion. Our risk-sensitive loss with $\lambda_{crit}=3.0$ provides the best balance between recall and overall accuracy.


\subsection{Qualitative Results}

\begin{figure}[!t]
  \centering
  \includegraphics[width=0.95\textwidth,height=0.32\textheight,keepaspectratio]{figures/figure3_pruning.png}
  \caption{\textbf{Token Pruning and Sparse Decoding Visualization.} (Left) Our spatial pruning preserves hazard-critical regions. (Right) FC-based importance scores accurately identify salient KV cache entries during decoding, focusing on safety-relevant tokens.}
  \label{fig:qualitative}
\end{figure}

Fig.~\ref{fig:qualitative} visualizes the combined effect. The spatial pruning preserves regions containing hazards while masking background. During decoding, the FC-based importance predictor focuses attention on the most safety-relevant entries in the KV cache, enabling detailed accident description generation.


% --- CONCLUSION ---
\section{Conclusion}
We introduced \textbf{Event-VLM}, a tri-axis inference framework for scalable surveillance VLMs. The core idea is to allocate computation only when needed (temporal gating), where needed (spatial pruning), and to what matters during generation (frequency-aware sparse decoding). This yields a coherent systems design rather than a collection of isolated accelerations.

Our risk-sensitive trigger preserves safety recall, adaptive pruning keeps hazard-critical context under aggressive token reduction, and FC-based decoding reduces KV-bandwidth pressure without retraining the backbone. Experiments on UCF-Crime and XD-Violence show \textbf{9$\times$} throughput gains while maintaining 99\% caption quality, indicating that high-fidelity accident explanation and real-time multi-stream operation can be achieved simultaneously.

\paragraph{Limitations and Future Work.}
Our framework relies on the initial detector's performance; however, the risk-sensitive loss achieves 98.2\% recall on critical events. The frequency-aware decoding currently uses a uniform FC budget across layers; future work will explore layer-adaptive budgets (cf.\ PyramidKV~\cite{pyramidkv}) and extend the frequency-domain analysis to visual encoders with RoPE (e.g., EVA-02). We also plan to validate on edge devices with integer quantization.


% ---- Bibliography ----
\bibliographystyle{splncs04}
\bibliography{main}


% ============================================
% APPENDIX
% ============================================
\appendix

\section{Implementation Details}
\label{sec:impl_details}

\subsection{Network Architecture}
\textbf{Trigger Module.} YOLOv8-Nano~\cite{yolo}: input 640$\times$640, CSPDarknet backbone (3.2M params), $\sim$1ms on RTX 5080.

\textbf{VLM Backbone.} LLaVA-1.5-7B with CLIP-ViT-L/14-336px visual encoder (576 patches) and Vicuna-7B LLM backbone (RoPE, $d=4096$, 32 heads, $d/2=64$ FCs per head).

\subsection{Hazard Class Taxonomy}
\begin{itemize}
    \item \textbf{Critical} ($\lambda_{crit}=3.0$): fire, smoke, explosion, structural\_collapse
    \item \textbf{High} ($\lambda_{high}=2.0$): forklift, crane, heavy\_machinery, falling\_object
    \item \textbf{Standard} ($\lambda_{std}=1.0$): person, vehicle, equipment
\end{itemize}

\subsection{Dominant FC Calibration Details}
The offline calibration uses 32 randomly sampled surveillance clips from the training set. For each attention head $(l,h)$ in the 32-layer Vicuna-7B, we compute CA scores for all 64 FCs with $\mathcal{K}=32$. We select $F=16$ dominant FCs per head. The entire calibration takes $<$5 minutes on a single GPU and is performed once.

\subsection{Training Details}
\textbf{Detector.} 100 epochs, SGD (momentum 0.937), lr 0.01 with cosine annealing, batch 16.

\textbf{Prompt Tuning.} 5 epochs, AdamW (lr 1e-4). Prompt length $K=8$ tokens for both banks.


\section{Additional Ablation Studies}
\label{sec:add_ablations}

\subsection{Hazard Weight Sensitivity}
\begin{table}[h]
\centering
\begin{tabular}{c|ccc}
\toprule
$\lambda_{crit}$ & Recall@Crit. & Prec.@Crit. & AUC \\
\midrule
1.0 & 91.2 & 88.5 & 84.2 \\
2.0 & 95.8 & 85.1 & 84.6 \\
3.0 (Ours) & 98.2 & 82.3 & 84.8 \\
4.0 & 99.1 & 78.9 & 83.9 \\
\bottomrule
\end{tabular}
\caption{Effect of critical hazard weight.}
\end{table}

\subsection{FC Budget ($F$) vs.\ Quality}
\begin{table}[h]
\centering
\begin{tabular}{c|ccc}
\toprule
$F$ (FCs) & \% of $d/2$ & CIDEr & Decode Speedup \\
\midrule
8 & 12.5\% & 87.5 & 2.8$\times$ \\
16 (Ours) & 25\% & 89.0 & 2.1$\times$ \\
32 & 50\% & 89.8 & 1.5$\times$ \\
64 (Full) & 100\% & 90.1 & 1.0$\times$ \\
\bottomrule
\end{tabular}
\caption{Trade-off between dominant FC budget and decoding quality/speed.}
\end{table}

\subsection{Adaptive Dilation Factor}
\begin{table}[h]
\centering
\begin{tabular}{c|ccc}
\toprule
$\beta$ & Fire CIDEr & Person CIDEr & Avg. Tokens \\
\midrule
0.0 (fixed) & 82.1 & 91.2 & 115 \\
0.5 & 86.4 & 90.8 & 128 \\
1.0 (Ours) & 89.5 & 90.1 & 142 \\
1.5 & 90.2 & 89.5 & 168 \\
\bottomrule
\end{tabular}
\caption{Adaptive dilation effect by hazard type.}
\end{table}


\section{Qualitative Examples}
\label{sec:qualitative_appendix}

\textbf{Fire Detection:}
\begin{itemize}
    \item \textit{Baseline}: ``There is smoke in the image.''
    \item \textit{Ours}: ``A fire has started near the storage area. The flames are spreading towards the east wall. Smoke is accumulating near the ceiling. Immediate evacuation recommended.''
\end{itemize}

\textbf{Forklift Accident:}
\begin{itemize}
    \item \textit{Baseline}: ``A person is lying on the ground near a vehicle.''
    \item \textit{Ours}: ``A worker has been struck by a forklift turning at the intersection. The worker appears unconscious. Medical assistance required immediately.''
\end{itemize}

\textbf{PPE Violation:}
\begin{itemize}
    \item \textit{Baseline}: ``Workers are present in the area.''
    \item \textit{Ours}: ``Two workers are operating near heavy machinery without proper safety helmets. Potential head injury risk. Safety protocol violation detected.''
\end{itemize}

\end{document}
